{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140044272168136\n",
      "140044271616136\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "num_gpus = 1\n",
    "l = tf.reshape(tf.tile(tf.range(100, dtype=tf.float32), [64]), [64, 100])\n",
    "l_by_gpu = tf.split(l, num_gpus)\n",
    "sums_by_gpu = [tf.zeros([]) for _ in range(num_gpus)]\n",
    "\n",
    "def body(sums_by_gpu, l_by_gpu, ref=None):\n",
    "    new_sum, new_l = [], []\n",
    "    for gpu_idx, (sum_, l) in enumerate(zip(ref[0], ref[1])):\n",
    "#         with tf.device('/gpu:{}'.format(gpu_idx)):\n",
    "#             sum_ += tf.reduce_sum(l[:, 0])\n",
    "#             l = tf.split(l, tf.stack([1, tf.shape(l)[-1]-1]), axis=1)[1]\n",
    "#             l = tf.Print(l, [tf.zeros([])])\n",
    "#             new_sum.append(sum_)\n",
    "#             new_l.append(l)\n",
    "        sum_ += tf.reduce_sum(l[:, 0])\n",
    "        l = tf.split(l, tf.stack([1, tf.shape(l)[-1]-1]), axis=1)[1]\n",
    "        l = tf.Print(l, [tf.zeros([])])\n",
    "        new_sum.append(sum_)\n",
    "        new_l.append(l)\n",
    "    return new_sum, new_l\n",
    "\n",
    "def cond(sums_by_gpu, l_by_gpu):\n",
    "    return tf.cast(tf.shape(l_by_gpu[0])[1], tf.bool)\n",
    "\n",
    "print(id(sums_by_gpu))\n",
    "sums, _ = tf.while_loop(\n",
    "    cond,\n",
    "    lambda x, y: body(x, y, ref=[sums_by_gpu, l_by_gpu]),\n",
    "    [sums_by_gpu, l_by_gpu],\n",
    "    shape_invariants=[\n",
    "        [sums_by_gpu[0].get_shape()]*len(sums_by_gpu),\n",
    "        [tf.TensorShape([None, None])]*len(l_by_gpu)\n",
    "    ]\n",
    ")\n",
    "print(id(sums))\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    sums = sess.run(sums)\n",
    "    print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.train import build_model_from_config\n",
    "from deeppavlov.core.data.dataset_reader import DatasetReader\n",
    "#from deeppavlov.core.data.utils import download_decompress\n",
    "from deeppavlov.core.common.registry import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10]\n",
      "0.0006940946914255619\n",
      "3.9800070226192474e-06\n",
      "5.837064236402512e-06\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "print(\n",
    "    timeit.timeit(\n",
    "        stmt='s.extend([5, 6, 7, 8, 9, 10]);print(s)',\n",
    "        setup='s = [1, 2, 3, 4]',\n",
    "        number=10,\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    timeit.timeit(\n",
    "        stmt='s += [5, 6, 7, 8, 9, 10]',\n",
    "        setup='s = [1, 2, 3, 4]',\n",
    "        number=10,\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    timeit.timeit(\n",
    "        stmt='s = s + [5, 6, 7, 8, 9, 10]',\n",
    "        setup='s = [1, 2, 3, 4]',\n",
    "        number=10,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0 0\n",
      "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_1_squash(borders, i):\n",
    "    print(borders, i)\n",
    "#     left, right = borders[0], borders[1]\n",
    "#     left_idx, right_idx = round(left), np.ceil(right)\n",
    "#     num_ones = right_idx - left_idx - 1\n",
    "#     if left == left_idx:\n",
    "#         num_ones += 1\n",
    "#     if right == right_idx:\n",
    "#         num_ones += 1\n",
    "#     indices = list(zip(range(left_idx, right_idx+1), [i]*(right_idx - left_idx + 1)))\n",
    "#     values = [np.ceil(left) - left] if left_idx < left else []\n",
    "#     values += [1] * num_ones\n",
    "#     if right != right_idx:\n",
    "#         values.append(right - round(right))\n",
    "#     return indices, values\n",
    "\n",
    "def squash_sparse_matrix(N, M):\n",
    "    factor = N / M\n",
    "    borders = np.stack([np.arange(N, 0, -1), np.arange(1, N+1)]) * factor\n",
    "    # print(borders.shape)\n",
    "    borders_sep = np.split(borders, borders.shape[0])\n",
    "    # print(borders_sep)\n",
    "    vfunc = np.vectorize(process_1_squash, otypes=[np.int32, np.float32])\n",
    "    return vfunc(borders, tuple(range(N)))\n",
    "    \n",
    "print(squash_sparse_matrix(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('напишите', 1), ('программу', 1), ('вычисляющую', 1), ('каждое', 1), ('встречается', 1), ('поданной', 1), ('на', 1), ('вход', 1), ('результаты', 1), ('подсчета', 1), ('поместите', 1), ('распечатайте', 1), ('порядке', 1), ('убывания', 1), ('слов', 1), ('каждой', 1), ('сначала', 1), ('печатается', 1), ('номер', 1), ('места', 1), ('по', 1), ('которое', 1), ('заняло', 1), ('через', 1), ('пробел', 1), ('то', 1), ('оно', 1), ('встретилось', 1), ('само', 1), ('гарантируется', 1), ('что', 1), ('во', 1), ('входном', 1), ('не', 1), ('будет', 1), ('символов', 1), ('кроме', 1), ('букв', 1), ('пробелов', 1), ('перед', 1), ('сортировкой', 1), ('весь', 1), ('текст', 1), ('следует', 1), ('перевести', 1), ('нижний', 1), ('регистр', 1), ('с', 1), ('помощью', 1), ('метода', 1), ('strlower', 1), ('сколько', 2), ('раз', 2), ('тексте', 2), ('строке', 2), ('словарь', 2), ('а', 2), ('затем', 2), ('встречаемости', 2), ('и', 2), ('слово', 3), ('в', 5)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Напишите программу вычисляющую сколько раз каждое слово \\\n",
    "встречается в поданной на вход тексте строке Результаты подсчета \\\n",
    "поместите в словарь а затем распечатайте словарь в порядке убывания \\\n",
    "встречаемости слов В каждой строке сначала печатается номер места \\\n",
    "по встречаемости которое заняло слово а затем через пробел то \\\n",
    "сколько раз оно встретилось и само слово  Гарантируется что во \\\n",
    "входном тексте не будет символов кроме букв и пробелов Перед \\\n",
    "сортировкой весь текст следует перевести в нижний регистр с помощью метода strlower\"\n",
    "m = text.lower().split()\n",
    "t = []\n",
    "d = {}\n",
    "for i in m:\n",
    "    t = m.count(i)\n",
    "    d[i] = t\n",
    "d1 = sorted(d.items(), key=lambda x: x[1])\n",
    "#print(t)\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def f(x, y):\n",
    "    return max(x, y)\n",
    "\n",
    "vecf = np.vectorize(f)\n",
    "\n",
    "print(vecf([i for i in range(10)], [i+1 for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "distr = [0.05, 0.30, 0.60, 0.05]\n",
    "N = len(distr)\n",
    "batch_size = 31\n",
    "\n",
    "cum_distr = distr[:1]\n",
    "for p in distr[1:]:\n",
    "    cum_distr.append(cum_distr[-1] + p)\n",
    "    \n",
    "cum_distr = tf.constant(cum_distr)\n",
    "\n",
    "x = tf.Variable(tf.truncated_normal([N]))\n",
    "\n",
    "unif = tf.random_uniform([batch_size, 1])\n",
    "comp = tf.to_float(cum_distr > unif)\n",
    "all_labels = tf.eye(N)\n",
    "indices = tf.argmax(comp, output_type=tf.int32, axis=-1)\n",
    "labels = tf.gather(all_labels, indices)\n",
    "probs = tf.nn.softmax(x)\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.stack([x]*batch_size), labels=labels)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.001, global_step, 1000, 0.5, staircase=True)\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.08096984 0.0975479  0.26694262 0.5545396 ]\n",
      "1000 [0.0533044  0.19900034 0.5111163  0.23657893]\n",
      "2000 [0.04506009 0.2585933  0.5747717  0.12157496]\n",
      "3000 [0.0462773  0.28025383 0.5934374  0.08003151]\n",
      "4000 [0.04730603 0.2972135  0.59320414 0.06227635]\n",
      "5000 [0.04869702 0.29630738 0.6007808  0.05421491]\n",
      "6000 [0.04989836 0.302922   0.5937062  0.05347345]\n",
      "7000 [0.04912981 0.30450043 0.59776264 0.04860713]\n",
      "8000 [0.05085437 0.301653   0.5983184  0.04917423]\n",
      "9000 [0.04980492 0.30537108 0.59524184 0.0495821 ]\n",
      "10000 [0.04901305 0.29486674 0.60645235 0.0496678 ]\n",
      "11000 [0.05039519 0.30151966 0.5997605  0.04832472]\n",
      "12000 [0.05130767 0.2991871  0.6008251  0.04868015]\n",
      "13000 [0.04810471 0.30305338 0.59956044 0.04928147]\n",
      "14000 [0.05046886 0.3014141  0.59800047 0.05011658]\n",
      "15000 [0.05116741 0.2969777  0.6018445  0.0500104 ]\n",
      "16000 [0.05030151 0.30790877 0.5929822  0.04880754]\n",
      "17000 [0.05044535 0.30435082 0.59567946 0.04952439]\n",
      "18000 [0.05145176 0.30772248 0.59086555 0.04996015]\n",
      "19000 [0.04869069 0.29502562 0.6079232  0.04836055]\n",
      "20000 [0.05054011 0.2997276  0.60078275 0.04894949]\n",
      "21000 [0.0499399  0.29681426 0.6044737  0.04877222]\n",
      "22000 [0.04995796 0.29944372 0.6013185  0.04927984]\n",
      "23000 [0.0506405  0.30111867 0.5986086  0.04963226]\n",
      "24000 [0.05093496 0.30018    0.597552   0.05133304]\n",
      "25000 [0.04989674 0.2978597  0.601153   0.05109052]\n",
      "26000 [0.05170452 0.2944559  0.6041289  0.04971063]\n",
      "27000 [0.05421989 0.29624635 0.5989833  0.05055047]\n",
      "28000 [0.05115524 0.30012408 0.600018   0.04870263]\n",
      "29000 [0.04835065 0.29969594 0.6011171  0.05083638]\n",
      "30000 [0.05016481 0.30092555 0.5978995  0.05101006]\n",
      "31000 [0.04891286 0.3030634  0.5987462  0.04927757]\n",
      "32000 [0.05196333 0.29684785 0.60149217 0.04969669]\n",
      "33000 [0.04924804 0.30571905 0.595665   0.04936797]\n",
      "34000 [0.05164746 0.30069527 0.5978199  0.0498373 ]\n",
      "35000 [0.0513721  0.29977235 0.5982852  0.05057042]\n",
      "36000 [0.04845599 0.2979999  0.6040262  0.04951799]\n",
      "37000 [0.04834603 0.30176088 0.5996025  0.05029061]\n",
      "38000 [0.04900131 0.2980474  0.60372746 0.04922383]\n",
      "39000 [0.05099601 0.29944015 0.6010774  0.04848645]\n",
      "40000 [0.05072872 0.2941191  0.6056601  0.04949213]\n",
      "41000 [0.05025207 0.30092472 0.59772885 0.05109432]\n",
      "42000 [0.04909933 0.29501805 0.60587883 0.05000375]\n",
      "43000 [0.05082985 0.29563934 0.60114515 0.05238574]\n",
      "44000 [0.05145663 0.29666933 0.6027223  0.04915173]\n",
      "45000 [0.05079317 0.29818144 0.6009019  0.05012346]\n",
      "46000 [0.05368599 0.3014779  0.5950613  0.04977484]\n",
      "47000 [0.04991604 0.29936203 0.6020362  0.04868572]\n",
      "48000 [0.04855623 0.30378383 0.59776324 0.04989668]\n",
      "49000 [0.0493275  0.29554284 0.6068535  0.04827616]\n",
      "50000 [0.05090529 0.29867187 0.60062927 0.04979357]\n",
      "51000 [0.0511696  0.2996637  0.59975696 0.04940976]\n",
      "52000 [0.05048654 0.29671186 0.6046492  0.0481524 ]\n",
      "53000 [0.0509831  0.30238923 0.5967039  0.04992382]\n",
      "54000 [0.05317304 0.29541644 0.6020789  0.04933161]\n",
      "55000 [0.04990523 0.30157566 0.5986632  0.04985591]\n",
      "56000 [0.05110686 0.30029267 0.5990843  0.04951617]\n",
      "57000 [0.04967197 0.2942415  0.60705084 0.04903567]\n",
      "58000 [0.05058382 0.3092297  0.5893407  0.05084575]\n",
      "59000 [0.05062233 0.2918072  0.606366   0.05120453]\n",
      "60000 [0.05022667 0.29692897 0.6030814  0.04976299]\n",
      "61000 [0.05134851 0.30378544 0.59574544 0.04912057]\n",
      "62000 [0.05106631 0.3052011  0.5940408  0.0496918 ]\n",
      "63000 [0.04802716 0.3066559  0.5967995  0.04851738]\n",
      "64000 [0.04926005 0.29218233 0.6085578  0.04999982]\n",
      "65000 [0.04884528 0.29757679 0.6029061  0.05067179]\n",
      "66000 [0.05026316 0.29399705 0.6073656  0.04837425]\n",
      "67000 [0.0526369  0.29606944 0.60169435 0.04959929]\n",
      "68000 [0.05082848 0.29251698 0.6062179  0.05043659]\n",
      "69000 [0.04992136 0.29590487 0.60175    0.05242375]\n",
      "70000 [0.04970565 0.29807296 0.6007158  0.05150557]\n",
      "71000 [0.04969887 0.29990885 0.5991264  0.05126585]\n",
      "72000 [0.0501684  0.3045173  0.59361976 0.05169454]\n",
      "73000 [0.05009573 0.30219218 0.5984387  0.04927335]\n",
      "74000 [0.0482243  0.30518076 0.59790546 0.0486894 ]\n",
      "75000 [0.04878746 0.3022386  0.59905577 0.04991814]\n",
      "76000 [0.04978348 0.29609746 0.6045171  0.04960196]\n",
      "77000 [0.048989   0.29876143 0.60221744 0.05003214]\n",
      "78000 [0.04994263 0.3037171  0.5968464  0.0494938 ]\n",
      "79000 [0.04941226 0.30650234 0.59286493 0.0512205 ]\n",
      "80000 [0.05128021 0.30328512 0.5935858  0.05184886]\n",
      "81000 [0.04886547 0.2988489  0.6017666  0.05051908]\n",
      "82000 [0.04958993 0.30075893 0.5998152  0.04983596]\n",
      "83000 [0.04960028 0.30025843 0.59812146 0.05201983]\n",
      "84000 [0.04954771 0.29948315 0.6015822  0.04938693]\n",
      "85000 [0.04886223 0.29590014 0.6049672  0.05027046]\n",
      "86000 [0.04961788 0.30070236 0.59881026 0.05086951]\n",
      "87000 [0.0512479  0.29600587 0.60326403 0.04948216]\n",
      "88000 [0.0503039  0.30096057 0.5988691  0.04986642]\n",
      "89000 [0.04995573 0.29954332 0.5999751  0.05052583]\n",
      "90000 [0.04902667 0.3045624  0.5987576  0.04765333]\n",
      "91000 [0.05009542 0.29786226 0.6017889  0.05025347]\n",
      "92000 [0.05071543 0.2998775  0.59992707 0.04948004]\n",
      "93000 [0.0509348  0.3038702  0.5951314  0.05006361]\n",
      "94000 [0.04880513 0.30722764 0.592068   0.05189927]\n",
      "95000 [0.05163822 0.29674324 0.60144526 0.05017327]\n",
      "96000 [0.05084784 0.30485427 0.5941563  0.05014158]\n",
      "97000 [0.04945128 0.2880891  0.6124611  0.04999853]\n",
      "98000 [0.04955704 0.3028433  0.5983879  0.04921177]\n",
      "99000 [0.04980431 0.29899228 0.6014033  0.04980011]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100000):\n",
    "        _, pr = sess.run([train_op, probs])\n",
    "#         print(cd)\n",
    "#         print(u)\n",
    "#         print(c)\n",
    "        if i % 1000 == 0:\n",
    "            print(i, pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 1, 2, 3], [0.5477225575051661, 0.5477225575051661, 0.5477225575051661, 0.18257418583505544])\n",
      "([3, 4, 5, 6], [0.36514837167011066, 0.5477225575051661, 0.5477225575051661, 0.3651483716701109])\n",
      "([6, 7, 8, 9], [0.1825741858350552, 0.5477225575051661, 0.5477225575051661, 0.5477225575051661])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def indices_and_weights_for_squeezing_of_1_neuron(i, N, M):\n",
    "    weights = []\n",
    "    indices = []\n",
    "    q = N / M\n",
    "    left, right = i * q, (i + 1) * q\n",
    "    if int(left) == left:\n",
    "        start_of_ones = int(left)\n",
    "    else:\n",
    "        weights.append(np.ceil(left) - left)\n",
    "        indices.append(int(left))\n",
    "        start_of_ones = int(left) + 1\n",
    "    \n",
    "    end_of_ones = int(right)\n",
    "    weights += [1.] * (end_of_ones - start_of_ones)\n",
    "    indices.extend(range(start_of_ones, end_of_ones))\n",
    "    \n",
    "    if int(right) != right:\n",
    "        weights.append(right - int(right))\n",
    "        indices.append(indices[-1] + 1)\n",
    "        \n",
    "    factor = q ** -0.5\n",
    "    weights = [w * factor for w in weights]\n",
    "    return indices, weights\n",
    "\n",
    "N = 10\n",
    "M = 3\n",
    "for i in range(M):\n",
    "    print(indices_and_weights_for_squeezing_of_1_neuron(i, N, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (4, 1), (5, 1), (6, 1), (6, 2), (7, 2), (8, 2), (9, 2)], [0.5477225575051661, 0.5477225575051661, 0.5477225575051661, 0.18257418583505544, 0.36514837167011066, 0.5477225575051661, 0.5477225575051661, 0.3651483716701109, 0.1825741858350552, 0.5477225575051661, 0.5477225575051661, 0.5477225575051661])\n"
     ]
    }
   ],
   "source": [
    "def squeezing_sparse_matrix(N, M):\n",
    "    init_indices, init_weights = [], []\n",
    "    for i in range(M):\n",
    "        indices, weights = indices_and_weights_for_squeezing_of_1_neuron(i, N, M)\n",
    "        init_indices.extend(zip(indices, [i] * len(indices)))\n",
    "        init_weights += weights\n",
    "    return init_indices, init_weights\n",
    "\n",
    "print(init_of_squeezing_sparse_matrix(N, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "v = tf.placeholder(tf.float32, shape=[1, None])\n",
    "r = tf.concat([v, tf.zeros([1, 2])], 0)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(r, feed_dict={v: [[1, 2]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-01 15:29:52.751 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/anton/dpenv/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-08-01 15:29:52.805 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abc'], ['def']]\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.tokenizers.ru_tokenizer import RussianTokenizer\n",
    "\n",
    "tokenizer = RussianTokenizer(ngram_range=[1, 1])\n",
    "batch = ['abc', 'def']\n",
    "print(tokenizer(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(<class 'tuple'>,)\n",
      "()\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "d = collections.OrderedDict(a=1, b=2)\n",
    "print(type(d) == collections.OrderedDict)\n",
    "\n",
    "tuple_type = collections.namedtuple('mytuple', [])\n",
    "\n",
    "t = tuple_type()\n",
    "print(tuple_type.__bases__)\n",
    "print(getattr(tuple_type, '_fields'))\n",
    "print(all([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, rnn1/transpose/_3, rnn1/concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, rnn2/transpose/_9, rnn2/transpose_1/_11, rnn2/concat_5/_13, rnn2/concat_11/_15)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-2f993b27c650>\", line 11, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1284, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 278, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 194, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1687, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, rnn1/transpose/_3, rnn1/concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, rnn2/transpose/_9, rnn2/transpose_1/_11, rnn2/concat_5/_13, rnn2/concat_11/_15)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, rnn1/transpose/_3, rnn1/concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, rnn2/transpose/_9, rnn2/transpose_1/_11, rnn2/concat_5/_13, rnn2/concat_11/_15)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2f993b27c650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1651\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, rnn1/transpose/_3, rnn1/concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, rnn2/transpose/_9, rnn2/transpose_1/_11, rnn2/concat_5/_13, rnn2/concat_11/_15)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-2f993b27c650>\", line 11, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1284, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 278, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 194, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1687, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, rnn1/transpose/_3, rnn1/concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, rnn2/transpose/_9, rnn2/transpose_1/_11, rnn2/concat_5/_13, rnn2/concat_11/_15)]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM as CudnnLSTM\n",
    "inp = tf.zeros([10, 32, 100])\n",
    "lstm1 = CudnnLSTM(1, 128)\n",
    "lstm2 = CudnnLSTM(2, 256)\n",
    "with tf.name_scope('rnn1'):\n",
    "    lstm1.build(inp.shape)\n",
    "with tf.name_scope('rnn2'):\n",
    "    lstm2.build(inp.shape)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    save_path = os.path.join('test_cudnn_lstm_save', '1')\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(os.path.join(save_path))\n",
    "    saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2, 2, 3, 4) (0, 0, 0, 1, 1, 1)\n",
      "[[0.66666667 0.        ]\n",
      " [0.66666667 0.        ]\n",
      " [0.33333333 0.33333333]\n",
      " [0.         0.66666667]\n",
      " [0.         0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def indices_and_weights_for_squeezing_of_1_neuron(i, N, M):\n",
    "    weights = []\n",
    "    indices = []\n",
    "    q = N / M\n",
    "    left, right = i * q, (i + 1) * q\n",
    "    if N >= M:\n",
    "        if int(left) == left:\n",
    "            start_of_ones = int(left)\n",
    "        else:\n",
    "            weights.append(np.ceil(left) - left)\n",
    "            indices.append(int(left))\n",
    "            start_of_ones = int(left) + 1\n",
    "\n",
    "        end_of_ones = int(right)\n",
    "        weights += [1.] * (end_of_ones - start_of_ones)\n",
    "        indices.extend(range(start_of_ones, end_of_ones))\n",
    "\n",
    "        if int(right) != right:\n",
    "            weights.append(right - int(right))\n",
    "            indices.append(indices[-1] + 1)\n",
    "    else:\n",
    "        left_floor, right_floor = int(left), int(right)\n",
    "        if left_floor == right_floor or right == right_floor:\n",
    "            weights.append(right - left)\n",
    "            indices.append(left_floor)\n",
    "        else:\n",
    "            weights += [right_floor - left, right - right_floor]\n",
    "            indices += [left_floor, right_floor]\n",
    "\n",
    "    factor = sum([w**2 for w in weights]) ** -0.5\n",
    "    weights = [w * factor for w in weights]\n",
    "    return indices, weights\n",
    "\n",
    "def squeezing_sparse_matrix(N, M):\n",
    "    init_indices, init_weights = [], []\n",
    "    for i in range(M):\n",
    "        # print(\"(util.tensor.squeezing_sparse_matrix)N, M:\", N, M)\n",
    "        indices, weights = indices_and_weights_for_squeezing_of_1_neuron(i, N, M)\n",
    "        init_indices.extend(zip(indices, [i] * len(indices)))\n",
    "        init_weights += weights\n",
    "    return init_indices, init_weights\n",
    "\n",
    "N, M = 5, 2\n",
    "indices, weights = squeezing_sparse_matrix(N, M)\n",
    "row_ids, col_ids = zip(*indices)\n",
    "print(row_ids, col_ids)\n",
    "sp_matrix = coo_matrix((weights, (row_ids, col_ids)), shape=(N, M))\n",
    "print(sp_matrix.todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you need the answer? (y/n): y\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'EssentialAnswers' has no attribute 'the_answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-22c1a5774edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEssentialAnswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthe_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPhilosopher1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthe_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPhilosopher2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEssentialAnswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'EssentialAnswers' has no attribute 'the_answer'"
     ]
    }
   ],
   "source": [
    "x = input(\"Do you need the answer? (y/n): \")\n",
    "if x.lower() == \"y\":\n",
    "    required = True\n",
    "else:\n",
    "    required = False\n",
    "    \n",
    "def the_answer(self, *args):              \n",
    "        return 42\n",
    "    \n",
    "class EssentialAnswers(type):\n",
    "    \n",
    "    def __init__(cls, clsname, superclasses, attributedict):\n",
    "        if required:\n",
    "            cls.the_answer = the_answer\n",
    "                           \n",
    "    \n",
    "class Philosopher1(metaclass=EssentialAnswers): \n",
    "    pass\n",
    "\n",
    "\n",
    "print(Philosopher1.the_answer)\n",
    "class Philosopher2(metaclass=EssentialAnswers): \n",
    "    pass\n",
    "class Philosopher3(metaclass=EssentialAnswers): \n",
    "    pass\n",
    "    \n",
    "    \n",
    "plato = Philosopher1()\n",
    "print(plato.the_answer())\n",
    "kant = Philosopher2()\n",
    "# let's see what Kant has to say :-)\n",
    "print(kant.the_answer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-03 15:36:34.385 INFO in 'char_lm_vocab'['char_lm_vocab'] at line 70: [loading vocabulary from /home/anton/DeepPavlov/download/vocab.dict]\n",
      "2018-08-03 15:36:34.400 INFO in 'char_lm_vocab'['char_lm_vocab'] at line 70: [loading vocabulary from /home/anton/DeepPavlov/download/vocab.dict]\n",
      "2018-08-03 15:36:34.412 INFO in 'char_lm_vocab'['char_lm_vocab'] at line 59: [saving vocabulary to /home/anton/DeepPavlov/download/vocab.dict]\n"
     ]
    }
   ],
   "source": [
    "from char_lm_vocab import CharLMVocabulary\n",
    "vocab = CharLMVocabulary(\n",
    "    save_path='./vocab.dict',\n",
    "    load_path='./vocab.dict',\n",
    ")\n",
    "text = 'DeepPavlov is an open-source conversational AI library built on TensorFlow and Keras.'\n",
    "vocab.fit(text)\n",
    "vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "r\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "[3, 14, None]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(vocab[5])\n",
    "print('z' in vocab)\n",
    "print('a' in vocab)\n",
    "print(vocab.is_str_batch([]))\n",
    "print(vocab.is_str_batch(['c']))\n",
    "print(vocab(['a', 'b', 'я']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99608877cb86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCudnnLSTM\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Bring in subpackages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeneratorEnqueuer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedEnqueuer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, tp)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Invokes __set__.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m_import_module\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m\"\"\"Import module, returning the module after the last dot.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM as LSTM\n",
    "import numpy as np\n",
    "\n",
    "num_units = 100\n",
    "voc_size = 20\n",
    "inps = tf.placeholder(tf.int32, shape=[None, None])\n",
    "lbls = tf.placeholder(tf.int32, shape=[None, None])\n",
    "prep_inps = tf.one_hot(inps, voc_size)\n",
    "prep_lbls = tf.one_hot(lbls, voc_size)\n",
    "\n",
    "out_w = tf.Variable(\n",
    "    tf.truncated_normal([num_units, voc_size])\n",
    ")\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    lstm = LSTM(\n",
    "        2,\n",
    "        num_units,\n",
    "        input_mode='skip_input',\n",
    "    )\n",
    "\n",
    "bs = tf.shape(inps)[1:-1]\n",
    "state_shape = tf.concat([[2], bs, [num_units]], 0)\n",
    "with tf.device('/gpu:0'):\n",
    "    lstm_res, state = lstm(\n",
    "        prep_inps\n",
    "    )\n",
    "    print(lstm_res)\n",
    "    logits = tf.matmul(tf.reshape(lstm_res, [-1, num_units]), out_w)\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits,\n",
    "        labels=tf.reshape(prep_lbls, [-1, voc_size])\n",
    "    ))\n",
    "    opt = tf.train.GradientDescentOptimizer(1.)\n",
    "    train_op = opt.minimize(loss)\n",
    "    \n",
    "nu = 10\n",
    "bs = 32\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(100):\n",
    "        inputs = np.random.randint(0, voc_size, size=(nu, bs))\n",
    "        labels = np.random.randint(0, voc_size, size=(nu, bs))\n",
    "        _, preds = sess.run([train_op, preds], feed_dict={inps: inputs, lbls: labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def create_distribute_map(num_distributed, result_length):\n",
    "    quotient = result_length / num_distributed\n",
    "    num_filled = 0\n",
    "    if num_distributed < result_length:\n",
    "        num_repeats = list()\n",
    "        for i in range(1, num_distributed):\n",
    "            filled_space = quotient * i\n",
    "            print(filled_space)\n",
    "            truncated = int(filled_space)\n",
    "            num_repeats.append(truncated - num_filled)\n",
    "            num_filled = truncated\n",
    "        num_repeats.append(result_length - num_filled)\n",
    "    else:\n",
    "        num_repeats = [1] * result_length\n",
    "    map_ = list(itertools.chain(*[[i] * n_rep for i, n_rep in enumerate(num_repeats)]))\n",
    "    return map_\n",
    "\n",
    "print(create_distribute_map(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function f at 0x7ff2b407ac80>\n",
      "<function <lambda> at 0x7ff2b407ad90>\n",
      "<function <lambda> at 0x7ff2b407a9d8>\n",
      "<function f at 0x7ff2b407ac80>\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    pass\n",
    "h = f\n",
    "g = lambda x: x\n",
    "i = lambda x: x\n",
    "print(h)\n",
    "print(g)\n",
    "print(i)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^Mama и ^Папа\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Mama и Папа\"\n",
    "text = re.sub(\"([A-ZА-Я])\", r\"^\\1\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^a component of ^tool's song repertoire relies on the use of unusual time signatures. ^for instance, ^chancellor describes the time signature employed on the first single from ^lateralus, \"^schism\", as \"six\" and \"six-and-a-half\" and that it later \"goes into all kinds of other times\".[115] ^further examples include the album's title track, which also displays shifting rhythms,[115] as do 10,000 ^days: \"^wings for ^marie (^pt 1)\" and \"10,000 ^days (^wings ^pt 2)\".[116]\n",
      "\n",
      "A component of Tool's song repertoire relies on the use of unusual time signatures. For instance, Chancellor describes the time signature employed on the first single from Lateralus, \"Schism\", as \"six\" and \"six-and-a-half\" and that it later \"goes into all kinds of other times\".[115] Further examples include the album's title track, which also displays shifting rhythms,[115] as do 10,000 Days: \"Wings for Marie (Pt 1)\" and \"10,000 Days (Wings Pt 2)\".[116]\n"
     ]
    }
   ],
   "source": [
    "from helmo.util.text import preprocessing, postprocessing\n",
    "text = \"A component of Tool's song repertoire relies on the use of unusual time signatures. For instance, Chancellor describes the time signature employed on the first single from Lateralus, \\\"Schism\\\", as \\\"six\\\" and \\\"six-and-a-half\\\" and that it later \\\"goes into all kinds of other times\\\".[115] Further examples include the album's title track, which also displays shifting rhythms,[115] as do 10,000 Days: \\\"Wings for Marie (Pt 1)\\\" and \\\"10,000 Days (Wings Pt 2)\\\".[116]\"\n",
    "text = preprocessing.hat_uppercase(text)\n",
    "print(text)\n",
    "print()\n",
    "text = postprocessing.hat_uppercase(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([{'loss': 0.8077297808527947, 'bpc': 1.2, 'perplexity': 4.011995053052902, 'accuracy': 0.7565445318222046}], [], ['0'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_tt_results(result_dir, required_metric_names):\n",
    "    folders = filter(lambda x: x.isdigit(), os.listdir(result_dir))\n",
    "    metrics = list()\n",
    "    launches_for_testing = list()\n",
    "    trained_launches = list()\n",
    "    for folder in folders:\n",
    "        one_launch_metrics = dict()\n",
    "        path_to_metrics = os.path.join(result_dir, folder, 'testing/results')\n",
    "        if os.path.exists(path_to_metrics):\n",
    "            for metric_file in os.listdir(path_to_metrics):\n",
    "                metric_name = metric_file.split('_')[0]\n",
    "                with open(os.path.join(path_to_metrics, metric_file), 'r') as f:\n",
    "                    one_launch_metrics[metric_name] = float(f.read())\n",
    "        if set(required_metric_names) == set(one_launch_metrics.keys()):\n",
    "            metrics.append(one_launch_metrics)\n",
    "            trained_launches.append(folder)\n",
    "        elif len(one_launch_metrics) > 0:\n",
    "            launches_for_testing.append(folder)\n",
    "            trained_launches.append(folder)\n",
    "    return metrics, launches_for_testing, trained_launches\n",
    "\n",
    "\n",
    "required_metric_names = ['accuracy', 'bpc', 'perplexity', 'loss']\n",
    "result_dir = 'expres/resrnn/char/tt/first'\n",
    "print(load_tt_results(result_dir, required_metric_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: [1, 2, 3, 4]\n",
      "\n",
      "L2: [10, 11, 12]\n",
      "L1 after 'L1[:2] = L2': [10, 11, 12, 3, 4]\n",
      "\n",
      "L3: [-3, -4]\n",
      "L1 after 'L1[1:2] = L3': [10, -3, -4, 12, 3, 4]\n",
      "\n",
      "L4: [5, 6, 7, 8]\n",
      "L1 after 'L1[3:3] = L4': [10, -3, -4, 5, 6, 7, 8, 12, 3, 4]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to assign sequence of size 2 to extended slice of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-84da205e65ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L1 after 'L1[3:3] = L4':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mL5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mL1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nL5:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L1 after 'L1[:-2:-1] = L5':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to assign sequence of size 2 to extended slice of size 1"
     ]
    }
   ],
   "source": [
    "L1 = [1, 2, 3, 4]\n",
    "print(\"L1:\", L1)\n",
    "L2 = [10, 11, 12]\n",
    "L1[:2] = L2\n",
    "print(\"\\nL2:\", L2)\n",
    "print(\"L1 after 'L1[:2] = L2':\", L1)\n",
    "L3 = [-3, -4]\n",
    "L1[1:2] = L3\n",
    "print(\"\\nL3:\", L3)\n",
    "print(\"L1 after 'L1[1:2] = L3':\", L1)\n",
    "L4 = [5, 6, 7, 8]\n",
    "L1[3:3] = L4\n",
    "print(\"\\nL4:\", L4)\n",
    "print(\"L1 after 'L1[3:3] = L4':\", L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "from useful_functions import all_combs\n",
    "\n",
    "def create_even_distribution(num_unique_items, num_slots):\n",
    "    \"\"\"Returns a list with number of slots taken by each item. If num_slots < num_unique_items not all items\n",
    "    will be distributed\"\"\"\n",
    "    items_per_slot_float = num_slots / num_unique_items\n",
    "    total_slots_filled_saved = 0\n",
    "    if num_unique_items < num_slots:\n",
    "        item_distribution = list()\n",
    "        for i in range(1, num_unique_items):\n",
    "            total_slots_float = items_per_slot_float * i\n",
    "            total_slots_filled = int(total_slots_float)\n",
    "            item_distribution.append(total_slots_filled - total_slots_filled_saved)\n",
    "            total_slots_filled_saved = total_slots_filled\n",
    "        item_distribution.append(num_slots - total_slots_filled_saved)\n",
    "    else:\n",
    "        item_distribution = [1] * num_slots\n",
    "    return item_distribution\n",
    "\n",
    "\n",
    "def create_distribute_map(num_unique_items, num_slots):\n",
    "    item_distribution = create_even_distribution(num_unique_items, num_slots)\n",
    "    map_ = list(itertools.chain(*[[i] * n_rep for i, n_rep in enumerate(item_distribution)]))\n",
    "    return map_\n",
    "\n",
    "\n",
    "def approx_mem_consumption(config):\n",
    "    # in MB for sequence_length=100, batch_size=128, num_layers=2, num_units=2000\n",
    "    # working for all lstm variants\n",
    "    base_consumption = 4500\n",
    "    consumption = base_consumption * \\\n",
    "        (config['num_units'] / 2000)**2 * \\\n",
    "        (config['num_layers'] / 2) * \\\n",
    "        (config['batch_size'] / 128) * \\\n",
    "        (config['sequence_length'] / 100)\n",
    "    return consumption\n",
    "\n",
    "\n",
    "def num_consequent_repeats(list_, idx):\n",
    "    current = list_[idx]\n",
    "    i = 1\n",
    "    while list_[idx+i] == current:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def ceil(a):\n",
    "    if int(a) != a:\n",
    "        return int(a) + 1\n",
    "    return int(a)\n",
    "\n",
    "\n",
    "def split_experiment_config_into_separate_measurement_configs(config):\n",
    "    seq_lens = make_list(config['sequence_length'])\n",
    "    batch_sizes = make_list(config['batch_size'])\n",
    "    num_layers = make_list(config['num_layers'])\n",
    "    num_units = make_list(config['num_units'])\n",
    "    combs = all_combs([seq_lens, batch_sizes, num_layers, num_units])\n",
    "    configs = list()\n",
    "    for comb in combs:\n",
    "        conf = copy.deepcopy(config)\n",
    "        conf['sequence_length'] = comb[0]\n",
    "        conf['batch_size'] = comb[1]\n",
    "        conf['num_layers'] = comb[2]\n",
    "        conf['num_units'] = comb[3]\n",
    "        configs.append(conf)\n",
    "    if config['num_repeats'] == 1:\n",
    "        return configs\n",
    "    else:\n",
    "        confs = list()\n",
    "        for conf in configs:\n",
    "            confs.extend([conf]*config['num_repeats'])\n",
    "        return confs\n",
    "\n",
    "\n",
    "def get_configs_run_in_parallel(configs, counter):\n",
    "    current_config = configs[counter]\n",
    "    max_num_in_parallel = current_config['memory_per_experiment'] / approx_mem_consumption(current_config)\n",
    "    num_repeats = num_consequent_repeats(configs, counter)\n",
    "    configs_to_process = configs[counter:counter+num_repeats]\n",
    "    num_launches_required = ceil(num_repeats / max_num_in_parallel)\n",
    "    distribution = create_even_distribution(num_launches_required, num_repeats)\n",
    "    list_of_launches = list()\n",
    "    pointer = 0\n",
    "    for num_proc in distribution:\n",
    "        list_of_launches.append(configs_to_process[pointer:pointer+num_proc])\n",
    "        pointer += num_proc\n",
    "    return list_of_launches, num_repeats\n",
    "\n",
    "\n",
    "def make_list(candidate):\n",
    "    if isinstance(candidate, list):\n",
    "        l = candidate\n",
    "    else:\n",
    "        l = [candidate]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"num_layers\": [3, 4],\n",
    "  \"num_units\": [1000, 2000],\n",
    "  \"sequence_length\": [50, 100],\n",
    "  \"batch_size\": [128],\n",
    "  \"save_path\": \"results/lstm_test\",\n",
    "  \"lstm_type\": \"cudnn_stacked\",\n",
    "  \"mode\": \"train\",\n",
    "  \"num_steps\": 500,\n",
    "  \"num_proc_in_pool\": 5,\n",
    "  \"num_repeats\": 10,\n",
    "  \"memory_per_experiment\": 7500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "\n",
      "\n",
      "1\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "configs = split_experiment_config_into_separate_measurement_configs(config)\n",
    "launches = get_configs_run_in_parallel(configs, 0)\n",
    "for idx, launch in enumerate(launches[0]):\n",
    "    print(idx)\n",
    "    for conf in launch:\n",
    "        print(conf)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843.75\n"
     ]
    }
   ],
   "source": [
    "print(approx_mem_consumption(configs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "14\n",
      "58\n",
      "242\n",
      "994\n",
      "4034\n",
      "16258\n",
      "65282\n",
      "261634\n",
      "1047554\n",
      "4192258\n",
      "16773122\n",
      "67100674\n",
      "268419074\n",
      "1073709058\n",
      "4294901762\n",
      "17179738114\n",
      "68719214594\n",
      "274877382658\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(2**(2*i) - 2**i + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "4\n",
      "8\n",
      "14\n",
      "22\n",
      "32\n",
      "44\n",
      "58\n",
      "74\n",
      "92\n",
      "112\n",
      "134\n",
      "158\n",
      "184\n",
      "212\n",
      "242\n",
      "274\n",
      "308\n",
      "344\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(i**2 - i + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState_1/LSTMCellZeroState/zeros:0' shape=(10, 100) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState_1/LSTMCellZeroState/zeros_1:0' shape=(10, 100) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState_1/LSTMCellZeroState_1/zeros:0' shape=(10, 200) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState_1/LSTMCellZeroState_1/zeros_1:0' shape=(10, 200) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState_1/LSTMCellZeroState_2/zeros:0' shape=(10, 300) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState_1/LSTMCellZeroState_2/zeros_1:0' shape=(10, 300) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.nn.rnn_cell import LSTMCell as LSTMCell\n",
    "lstms = [LSTMCell(num_units, dtype=tf.float32) for num_units in [100, 200, 300]]\n",
    "multilayer_lstm = tf.contrib.rnn.MultiRNNCell(lstms)\n",
    "print(multilayer_lstm.zero_state(10, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.Variable(0, trainable=False, validate_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "b = ([1], [2], [3])\n",
    "for k in zip(*b):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_zip(objects, depth, passable_types=(list, tuple, dict)):\n",
    "    if depth != 0 and isinstance(objects[0], passable_types):\n",
    "        if isinstance(objects[0], (list, tuple)):\n",
    "            zipped = list()\n",
    "            for comb in zip(*objects):\n",
    "                zipped.append(\n",
    "                    deep_zip(comb, depth-1, passable_types=passable_types)\n",
    "                )\n",
    "            if isinstance(objects[0], tuple):\n",
    "                zipped = tuple(zipped)\n",
    "            return zipped\n",
    "        elif isinstance(objects[0], dict):\n",
    "            zipped = dict()\n",
    "            for key in objects[0].keys():\n",
    "                values = [obj[key] for obj in objects]\n",
    "                zipped[key] = deep_zip(values, depth-1, passable_types=passable_types)\n",
    "            return zipped\n",
    "    return tuple(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1, 5), (2, 6)), ([(3, 7), (9, 10)], (4, 8)), ({'a': 4}, {'a': 5})]\n"
     ]
    }
   ],
   "source": [
    "objects = [\n",
    "    [(1, 2), ([3, 9], 4), {'a': 4}],\n",
    "    [(5, 6), ([7, 10], 8), {'a': 5}],\n",
    "]\n",
    "depth = 10\n",
    "print(deep_zip(objects, depth, passable_types=(list, tuple)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_func_on_depth(obj, func, depth, passable_types=(list, tuple, dict)):\n",
    "    if depth != 0 and isinstance(obj, passable_types):\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            processed = list()\n",
    "            for elem in obj:\n",
    "                processed.append(apply_func_on_depth(elem, func, depth-1, passable_types=passable_types))\n",
    "            if isinstance(obj, tuple):\n",
    "                processed = tuple(processed)\n",
    "            return processed\n",
    "        elif isinstance(obj, dict):\n",
    "            processed = dict()\n",
    "            for key, value in obj.items():\n",
    "                processed[key] = apply_func_on_depth(value, func, depth-1, passable_types=passable_types)\n",
    "            return processed\n",
    "    return func(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 4), ([9, 81], 16), {'a': 16}], [(25, 36), ([49, 100], 64), {'a': 25}]]\n"
     ]
    }
   ],
   "source": [
    "obj = [\n",
    "    [(1, 2), ([3, 9], 4), {'a': 4}],\n",
    "    [(5, 6), ([7, 10], 8), {'a': 5}],\n",
    "]\n",
    "\n",
    "func = lambda x: x**2\n",
    "depth = 4\n",
    "print(apply_func_on_depth(obj, func, depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.Variable(0)\n",
    "ndim_tensor = tf.shape(tf.shape(a))\n",
    "is_scalar = tf.equal(tf.shape(tf.shape(a))[0], 0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(is_scalar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "f = lambda: 1\n",
    "print(f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('a', 1), ('b', 2), ('c', 3))\n"
     ]
    }
   ],
   "source": [
    "a = {'a': 1, 'b': 2, 'c': 3}\n",
    "def f(*args):\n",
    "    print(args)\n",
    "\n",
    "f(*a.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f() got multiple values for argument 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-672cbc5cc0b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: f() got multiple values for argument 'a'"
     ]
    }
   ],
   "source": [
    "def f(a=1):\n",
    "    print(a)\n",
    "    \n",
    "kwargs = {'a': 2}\n",
    "\n",
    "f(4, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7d0be93e607d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "d = {([1, 2], [3, 4]): 1, ([5, 6], [7, 8]): 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(not 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f1aa3bad00e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "a = [1, 2]\n",
    "print(a['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "\n",
      "True\n",
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\n",
    "# no exception\n",
    "sv = SimpleVocabulary(\n",
    "    load_path=\"file_which_does_not_exists\",\n",
    "    save_path=\"file_which_does_not_exists\"\n",
    ")\n",
    "print(sv.load_path.is_file())\n",
    "sv.load()\n",
    "print(len(sv))\n",
    "\n",
    "print()\n",
    "# exception\n",
    "sv = SimpleVocabulary(\n",
    "    load_path=\"no_such_dir/file_which_does_not_exists\",\n",
    "    save_path=\"no_such_dir/file_which_does_not_exists\"\n",
    ")\n",
    "print(sv.load_path.parent.is_dir())\n",
    "print(sv.load_path.is_file())\n",
    "sv.load()\n",
    "print(len(sv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    user_paths = os.environ['PYTHONPATH'].split(os.pathsep)\n",
    "except KeyError:\n",
    "    user_paths = []\n",
    "print(user_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (2, 2), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "l = [1, 2, 3, 2, 1, 1]\n",
    "freqs = Counter(chain(l))\n",
    "print(list(freqs.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'abc\\ndef'\n",
      "abc\n",
      "def\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s0 = 'abc\\ndef'\n",
    "s1 = repr(s0)\n",
    "print(s1)\n",
    "s2 = bytes(s, \"utf-8\").decode(\"unicode_escape\")\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a: int) -> int:\n",
    "    print(a)\n",
    "    return [3]\n",
    "    \n",
    "f([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccaccbaaba\n",
      "str32\n",
      "str\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5311f1a67660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab = ['a', 'b', 'c']\n",
    "a = np.random.rand(10, 3)\n",
    "def f(vec):\n",
    "    return vocab[np.argmax(vec)]\n",
    "b = np.apply_along_axis(f, -1, a)\n",
    "print(''.join(b))\n",
    "print(b.dtype.name)\n",
    "print(np.dtype(str).name)\n",
    "print(np.dtype(np.array(['a', 'b'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([['a', 'b']])\n",
    "print(a.dtype.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "a\n",
      "b\n",
      "('a', 'a')\n",
      "('b', 'b')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.nn.rnn_cell import LSTMStateTuple as LSTMStateTuple\n",
    "t1 = LSTMStateTuple(h='b', c='a')\n",
    "t2 = LSTMStateTuple(c='a', h='b')\n",
    "print(isinstance(t1, tuple))\n",
    "for e in t1:\n",
    "    print(e)\n",
    "for e in zip(t1, t2):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-13 15:26:54.204 WARNING in 'tensorflow'['tf_logging'] at line 120: <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f8bfc62b940>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "var and delta do not have the same shape[10,20] []\n\t [[Node: GradientDescent/update_state/ApplyGradientDescent = ApplyGradientDescent[T=DT_FLOAT, use_locking=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](state, GradientDescent/learning_rate, gradients/cond/Reshape/Switch_grad/cond_grad)]]\n\nCaused by op 'GradientDescent/update_state/ApplyGradientDescent', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-21777d4be57d>\", line 76, in <module>\n    train_op = opt.apply_gradients(grads_and_vars)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 605, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 114, in update_op\n    update_op = optimizer._apply_dense(g, self._v)  # pylint: disable=protected-access\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/gradient_descent.py\", line 60, in _apply_dense\n    use_locking=self._use_locking).op\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/gen_training_ops.py\", line 576, in apply_gradient_descent\n    use_locking=use_locking, name=name)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): var and delta do not have the same shape[10,20] []\n\t [[Node: GradientDescent/update_state/ApplyGradientDescent = ApplyGradientDescent[T=DT_FLOAT, use_locking=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](state, GradientDescent/learning_rate, gradients/cond/Reshape/Switch_grad/cond_grad)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: var and delta do not have the same shape[10,20] []\n\t [[Node: GradientDescent/update_state/ApplyGradientDescent = ApplyGradientDescent[T=DT_FLOAT, use_locking=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](state, GradientDescent/learning_rate, gradients/cond/Reshape/Switch_grad/cond_grad)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-21777d4be57d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: var and delta do not have the same shape[10,20] []\n\t [[Node: GradientDescent/update_state/ApplyGradientDescent = ApplyGradientDescent[T=DT_FLOAT, use_locking=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](state, GradientDescent/learning_rate, gradients/cond/Reshape/Switch_grad/cond_grad)]]\n\nCaused by op 'GradientDescent/update_state/ApplyGradientDescent', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-21777d4be57d>\", line 76, in <module>\n    train_op = opt.apply_gradients(grads_and_vars)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 605, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 114, in update_op\n    update_op = optimizer._apply_dense(g, self._v)  # pylint: disable=protected-access\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/gradient_descent.py\", line 60, in _apply_dense\n    use_locking=self._use_locking).op\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/gen_training_ops.py\", line 576, in apply_gradient_descent\n    use_locking=use_locking, name=name)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): var and delta do not have the same shape[10,20] []\n\t [[Node: GradientDescent/update_state/ApplyGradientDescent = ApplyGradientDescent[T=DT_FLOAT, use_locking=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](state, GradientDescent/learning_rate, gradients/cond/Reshape/Switch_grad/cond_grad)]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def is_scalar_tensor(t):\n",
    "    return tf.equal(tf.shape(tf.shape(t))[0], 0)\n",
    "\n",
    "\n",
    "def replace_empty_saved_state(saved_and_zero_state):\n",
    "    return tf.cond(\n",
    "        is_scalar_tensor(saved_and_zero_state[0]),\n",
    "        true_fn=lambda: saved_and_zero_state[1],\n",
    "        false_fn=lambda: tf.reshape(saved_and_zero_state[0], tf.shape(saved_and_zero_state[1])),\n",
    "    )\n",
    "\n",
    "def synchronous_flatten(*nested):\n",
    "    if not isinstance(nested[0], (tuple, list, dict)):\n",
    "        return [[n] for n in nested]\n",
    "    output = [list() for _ in nested]\n",
    "    if isinstance(nested[0], dict):\n",
    "        for k in nested[0].keys():\n",
    "            flattened = synchronous_flatten(*[n[k] for n in nested])\n",
    "            for o, f in zip(output, flattened):\n",
    "                o.extend(f)\n",
    "    else:\n",
    "        try:\n",
    "            for inner_nested in zip(*nested):\n",
    "                flattened = synchronous_flatten(*inner_nested)\n",
    "                for o, f in zip(output, flattened):\n",
    "                    o.extend(f)\n",
    "        except TypeError:\n",
    "            print('(synchronous_flatten)nested:', nested)\n",
    "            raise\n",
    "    return output\n",
    "\n",
    "def compose_save_list(*pairs, name_scope='save_list'):\n",
    "    with tf.name_scope(name_scope):\n",
    "        save_list = list()\n",
    "        for pair in pairs:\n",
    "            # print('pair:', pair)\n",
    "            [variables, new_values] = synchronous_flatten(pair[0], pair[1])\n",
    "            # print(\"(useful_functions.compose_save_list)variables:\", variables)\n",
    "            # variables = flatten(pair[0])\n",
    "            # # print(variables)\n",
    "            # new_values = flatten(pair[1])\n",
    "            for variable, value in zip(variables, new_values):\n",
    "                save_list.append(tf.assign(variable, value, validate_shape=False))\n",
    "        return save_list\n",
    "\n",
    "# saved_state = (\n",
    "#     tf.Variable(0., validate_shape=False, trainable=False, name='state'),\n",
    "#     tf.Variable(0., validate_shape=False, trainable=False, name='state')\n",
    "# )\n",
    "saved_state = tf.Variable(0., validate_shape=False, trainable=False, name='state')\n",
    "\n",
    "lstm = tf.nn.rnn_cell.LSTMCell(10, state_is_tuple=False)\n",
    "\n",
    "inp = tf.zeros([100, 10, 13])\n",
    "zero_state = lstm.zero_state(tf.shape(inp)[1], tf.float32)\n",
    "\n",
    "# state = [\n",
    "#     replace_empty_saved_state([s, z]) for s, z in zip(saved_state, zero_state)\n",
    "# ]\n",
    "state = replace_empty_saved_state([saved_state, zero_state])\n",
    "output, new_state = tf.nn.dynamic_rnn(\n",
    "    lstm, inp, initial_state=state, parallel_iterations=1024, time_major=True\n",
    ")\n",
    "\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(1.)\n",
    "\n",
    "save_list = compose_save_list((saved_state, new_state))\n",
    "with tf.control_dependencies(save_list):\n",
    "    loss = tf.reduce_sum(output)\n",
    "    grads_and_vars = opt.compute_gradients(loss, var_list=tf.global_variables())\n",
    "    grads_and_vars = [(grad, var) for grad, var in grads_and_vars if grad is not None]\n",
    "    train_op = opt.apply_gradients(grads_and_vars)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        sess.run(train_op)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "object\n",
      "object\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "indices_batch = np.array([None, None])\n",
    "print(indices_batch[0] is None)\n",
    "a = np.array([1, 2])\n",
    "for i in np.nditer(indices_batch, flags=['refs_ok']):\n",
    "    print(i.dtype.name)\n",
    "for i in np.nditer(a, flags=['refs_ok']):\n",
    "    print(i.dtype.name)\n",
    "if any([i is None for i in np.nditer(indices_batch, flags=['refs_ok'])]):\n",
    "    print('!')\n",
    "    g = np.vectorize(lambda x: x is None)\n",
    "    mask = g(indices_batch)\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'\n"
     ]
    }
   ],
   "source": [
    "print(repr('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]] [[1 4]\n",
      " [2 5]\n",
      " [3 6]] \n",
      "\n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]] [[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]] \n",
      "\n",
      "[[2 2]\n",
      " [2 2]\n",
      " [2 2]] \n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[[1, 4], [2, 5], [3, 6]], [[7, 8], [9, 10], [11, 12]]])\n",
    "b = np.reshape(a, (-1, a.shape[-1]))\n",
    "correct = sum([y1 == y2 for y1, y2 in zip(a, a)])\n",
    "for y1, y2 in zip(a, a):\n",
    "    print(y1, y2, '\\n')\n",
    "print(correct, '\\n')\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from deeppavlov.core.common.log import get_logger\n",
    "\n",
    "\n",
    "def prepare_for_accuracy_computation(y_true, y_predicted):\n",
    "    log = get_logger(__name__)\n",
    "    if not isinstance(y_true, np.ndarray):\n",
    "        y_true = np.array(y_true)\n",
    "    if not isinstance(y_predicted, np.ndarray):\n",
    "        y_predicted = np.array(y_predicted)\n",
    "    num_examples = y_true.size\n",
    "    if y_true.size != y_predicted.size:\n",
    "        num_examples = min(y_true.size, y_predicted.size)\n",
    "        log.warning(\n",
    "            \"Number of elements in y_true and in y_predicted are not equal\\n\"\n",
    "            \"y_true.size = {}, y_predicted.size = {}\\n\"\n",
    "            \"Using first {} elements of y_true and y_predicted\".format(\n",
    "                y_true.size, y_predicted.size, num_examples,\n",
    "            )\n",
    "        )\n",
    "        y_true, y_predicted = np.reshape(y_true, (-1)), np.reshape(y_predicted, (-1))\n",
    "        y_true, y_predicted = y_true[:num_examples], y_predicted[:num_examples]\n",
    "    if y_true.shape != y_predicted.shape:\n",
    "        log.warning(\n",
    "            \"y_true and y_predicted have different shapes\\ny_true.shape = {}, y_predicted.shape = {}\\n\"\n",
    "            \"Reshaping y_true to y_predicted.shape\".format(y_true.shape, y_predicted.shape)\n",
    "        )\n",
    "        y_true = np.reshape(y_true, y_predicted.shape)\n",
    "    return y_true, y_predicted, num_examples\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    Calculate accuracy in terms of absolute coincidence\n",
    "\n",
    "    Args:\n",
    "        y_true: array of true values\n",
    "        y_predicted: array of predicted values\n",
    "\n",
    "    Returns:\n",
    "        portion of absolutely coincidental samples\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true, y_predicted, num_examples = prepare_for_accuracy_computation(y_true, y_predicted)\n",
    "    correct = np.sum(y_true == y_predicted)\n",
    "    return correct / num_examples if num_examples else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-13 14:25:53.122 WARNING in '__main__'['<ipython-input-35-9e32c49161a5>'] at line 22: Number of elements in y_true and in y_predicted are not equal\n",
      "y_true.size = 3, y_predicted.size = 0\n",
      "Using first 0 elements of y_true and y_predicted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[1, 2, 4]])\n",
    "y_predicted = []\n",
    "print(accuracy(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/dpenv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/anton/dpenv/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.array([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "l = []\n",
    "l += list(a)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "a\n",
      "<class 'numpy.ndarray'>\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array(['a', 'b'])\n",
    "for e in np.nditer(a):\n",
    "    print(type(e))\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    global tf\n",
    "    import tensorflow as tf\n",
    "f()   \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"cudnn_lstm_1/CudnnRNN:0\", shape=(?, ?, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM as CudnnLSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "lstm = CudnnLSTM(1, 100)\n",
    "inp = tf.placeholder(shape=[None, None, 31], dtype=tf.float32)\n",
    "output, state = lstm(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'split:0' shape=(100, 50) dtype=float32>, <tf.Tensor 'split:1' shape=(0, 50) dtype=float32>]\n",
      "Tensor(\"MatMul_2:0\", shape=(0, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.zeros([0, 100])\n",
    "b = tf.ones([100, 50])\n",
    "d = tf.split(b, [100, 0], axis=0)\n",
    "print(d)\n",
    "c = tf.matmul(a, b)\n",
    "print(c)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import turtle\n",
    "turtle.shape('turtle')\n",
    "a = 50\n",
    "for i in range(360):\n",
    "    t = i * math.pi/180\n",
    "    x = 2*a*math.cos(t) - a*math.cos(2*t) - a\n",
    "    y = 2*a*math.sin(t) - a*math.sin(2*t)\n",
    "    turtle.goto(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid reduction dimension 4 for input with 3 dimensions. for 'Mean_2' (op: 'Mean') with input shapes: [100,100,100], [] and with computed input tensors: input[1] = <4>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1588\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Invalid reduction dimension 4 for input with 3 dimensions. for 'Mean_2' (op: 'Mean') with input shapes: [100,100,100], [] and with computed input tensors: input[1] = <4>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-26f2e4bc7d6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 instructions)\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)\u001b[0m\n\u001b[1;32m   1451\u001b[0m                                                   reduction_indices),\n\u001b[1;32m   1452\u001b[0m                                    \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                    name=name))\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   4494\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4495\u001b[0m         \u001b[0;34m\"Mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4496\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4497\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4498\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3412\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3413\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3414\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1754\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1755\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1756\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1757\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid reduction dimension 4 for input with 3 dimensions. for 'Mean_2' (op: 'Mean') with input shapes: [100,100,100], [] and with computed input tensors: input[1] = <4>."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.zeros([100, 100, 100])\n",
    "b = tf.reduce_mean(a, axis=tf.constant([1, 2]))\n",
    "c = tf.reduce_mean(a, axis=4)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import turtle\n",
    "turtle.shape('turtle')\n",
    "turtle.forward(50)\n",
    "turtle.left(90)\n",
    "turtle.forward(50)\n",
    "turtle.left(90)\n",
    "turtle.forward(50)\n",
    "turtle.right(90)\n",
    "turtle.forward(50)\n",
    "turtle.right(90)\n",
    "turtle.forward(50)\n",
    "turtle.right(90)\n",
    "turtle.forward(50)\n",
    "turtle.right(90)\n",
    "turtle.forward(50)\n",
    "turtle.left(90)\n",
    "turtle.forward(50)\n",
    "turtle.left(90)\n",
    "turtle.forward(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "turtle.shape('turtle')\n",
    "turtle.forward(50)\n",
    "turtle.left(90)\n",
    "turtle.forward(50)\n",
    "turtle.left(90)\n",
    "turtle.forward(50)\n",
    "turtle.right(90)\n",
    "turtle.forward(50)\n",
    "turtle.right(90)\n",
    "turtle.forward(50)\n",
    "turtle.right(90)\n",
    "turtle.forward(50)\n",
    "turtle.right(90)\n",
    "turtle.forward(50)\n",
    "turtle.left(90)\n",
    "turtle.forward(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import turtle\n",
    "x = 0\n",
    "y = 0\n",
    "a = 100\n",
    "turtle.reset\n",
    "turtle.down\n",
    "for i in range(0, 360):\n",
    "    x = 2*a*math.cos(math.radians(i))-a*math.cos(2*math.radians(i))\n",
    "    y = 2*a*math.sin(math.radians(i))-a*math.sin(2*math.radians(i))\n",
    "    turtle.goto(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import turtle\n",
    "x = 0\n",
    "y = 0\n",
    "a = 100\n",
    "turtle.reset\n",
    "turtle.down\n",
    "for i in range(0, 360):\n",
    "    x = 2*a*math.cos(math.radians(i))-a*math.cos(2*math.radians(i))\n",
    "    y = 2*a*math.sin(math.radians(i))-a*math.sin(2*math.radians(i))\n",
    "    turtle.goto(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "введите число 0=exit : 100\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
      "[1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f=True\n",
    "\n",
    "while f:\n",
    "    n=int(input('введите число 0=exit : '))\n",
    "    if n==0: break\n",
    "    a= [i for i in range(n+1)]\n",
    "    b=[]\n",
    "    print (a)\n",
    "\n",
    "    for i in range(2,n):\n",
    "        if (i**2<=n) and (a[i]!=0):\n",
    "            for j in range(i**2,n+1,i):\n",
    "                a[j]=0\n",
    "            #print(a)    \n",
    "    for i in range(n+1):\n",
    "        if a[i]!=0:\n",
    "            b.append(a[i])              \n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation 'ArgMax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT64]\n\n\t [[Node: ArgMax = ArgMax[T=DT_INT32, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/device:GPU:0\"](Placeholder, ArgMax/dimension)]]\n\nCaused by op 'ArgMax', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-565a258acdcc>\", line 5, in <module>\n    b = tf.argmax(a, axis=-1)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 88, in argmax\n    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 783, in arg_max\n    name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'ArgMax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT64]\n\n\t [[Node: ArgMax = ArgMax[T=DT_INT32, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/device:GPU:0\"](Placeholder, ArgMax/dimension)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'ArgMax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT64]\n\n\t [[Node: ArgMax = ArgMax[T=DT_INT32, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/device:GPU:0\"](Placeholder, ArgMax/dimension)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-565a258acdcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'ArgMax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT64]\n\n\t [[Node: ArgMax = ArgMax[T=DT_INT32, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/device:GPU:0\"](Placeholder, ArgMax/dimension)]]\n\nCaused by op 'ArgMax', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-565a258acdcc>\", line 5, in <module>\n    b = tf.argmax(a, axis=-1)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 88, in argmax\n    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 783, in arg_max\n    name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'ArgMax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_FLOAT]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT32]; Tidx in [DT_INT32]\n  device='GPU'; T in [DT_HALF]; output_type in [DT_INT64]; Tidx in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT64]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT32]\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT64]\n\n\t [[Node: ArgMax = ArgMax[T=DT_INT32, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/device:GPU:0\"](Placeholder, ArgMax/dimension)]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.placeholder(tf.int32)\n",
    "    b = tf.argmax(a, axis=-1)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(b, feed_dict={a: np.zeros(shape=(10, 20, 100), dtype=np.int32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [], 'b': []}\n"
     ]
    }
   ],
   "source": [
    "d = {k: list() for k in {'a': 1, 'b': 2}.keys()}\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value recall_1/true_positives/count\n\t [[Node: recall_1/true_positives/count/read = Identity[T=DT_FLOAT, _class=[\"loc:@recall_1/true_positives/AssignAdd\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](recall_1/true_positives/count)]]\n\t [[Node: recall_1/value/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_58_recall_1/value\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'recall_1/true_positives/count/read', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-9fd5e458e749>\", line 5, in <module>\n    recall=tf.metrics.recall(b,a)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\", line 2106, in recall\n    name=None)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\", line 1807, in true_positives\n    updates_collections)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\", line 1407, in _count_condition\n    count = metric_variable([], dtypes.float32, name='count')\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\", line 51, in metric_variable\n    name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2234, in variable\n    use_resource=use_resource)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2224, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2207, in default_variable_creator\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 422, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 79, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3263, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value recall_1/true_positives/count\n\t [[Node: recall_1/true_positives/count/read = Identity[T=DT_FLOAT, _class=[\"loc:@recall_1/true_positives/AssignAdd\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](recall_1/true_positives/count)]]\n\t [[Node: recall_1/value/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_58_recall_1/value\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value recall_1/true_positives/count\n\t [[Node: recall_1/true_positives/count/read = Identity[T=DT_FLOAT, _class=[\"loc:@recall_1/true_positives/AssignAdd\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](recall_1/true_positives/count)]]\n\t [[Node: recall_1/value/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_58_recall_1/value\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9fd5e458e749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minit_g\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value recall_1/true_positives/count\n\t [[Node: recall_1/true_positives/count/read = Identity[T=DT_FLOAT, _class=[\"loc:@recall_1/true_positives/AssignAdd\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](recall_1/true_positives/count)]]\n\t [[Node: recall_1/value/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_58_recall_1/value\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'recall_1/true_positives/count/read', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-9fd5e458e749>\", line 5, in <module>\n    recall=tf.metrics.recall(b,a)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\", line 2106, in recall\n    name=None)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\", line 1807, in true_positives\n    updates_collections)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\", line 1407, in _count_condition\n    count = metric_variable([], dtypes.float32, name='count')\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py\", line 51, in metric_variable\n    name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2234, in variable\n    use_resource=use_resource)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2224, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2207, in default_variable_creator\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 422, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 79, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3263, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value recall_1/true_positives/count\n\t [[Node: recall_1/true_positives/count/read = Identity[T=DT_FLOAT, _class=[\"loc:@recall_1/true_positives/AssignAdd\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](recall_1/true_positives/count)]]\n\t [[Node: recall_1/value/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_58_recall_1/value\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a=tf.Variable(tf.constant([0.,1.,2.]))\n",
    "b=tf.Variable(tf.constant([1.,1.,1.]))\n",
    "recall=tf.metrics.recall(b,a)\n",
    "\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init_g])\n",
    "    rec=sess.run(recall)\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "d = {'a': 1}\n",
    "print(hasattr(d, '__setitem__'))\n",
    "l = []\n",
    "print(hasattr(l, 'append'))\n",
    "print(hasattr(l, '__getitem__'))\n",
    "print(hasattr(l, '__setitem__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "d = OrderedDict([('a', 1)])\n",
    "print(isinstance(d, dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parent directory of test_cudnn_lstm_save doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/transpose/_1, cudnn_lstm/concat_5/_3, cudnn_lstm_1/transpose/_5, cudnn_lstm_1/concat_5/_7)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/transpose/_1, cudnn_lstm/concat_5/_3, cudnn_lstm_1/transpose/_5, cudnn_lstm_1/concat_5/_7)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-acc1ee31cbc9>\", line 10, in <module>\n    [lstm1.saveable, lstm2.saveable]\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1284, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 278, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 194, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1687, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/transpose/_1, cudnn_lstm/concat_5/_3, cudnn_lstm_1/transpose/_5, cudnn_lstm_1/concat_5/_7)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-acc1ee31cbc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[1;32m   1668\u001b[0m                   save_path))\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of test_cudnn_lstm_save doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM as CudnnLSTM\n",
    "inp = tf.zeros([10, 32, 100])\n",
    "lstm1 = CudnnLSTM(1, 128)\n",
    "lstm2 = CudnnLSTM(1, 256)\n",
    "_, _ = lstm1(inp)\n",
    "_, _ = lstm2(inp)\n",
    "saver = tf.train.Saver(\n",
    "    [lstm1.saveable, lstm2.saveable]\n",
    ")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    save_path = 'test_cudnn_lstm_save'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print([1, 2] == (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400000\n"
     ]
    }
   ],
   "source": [
    "print(int(6.4e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anton/h-elmo/helmo/experiments/test.py\n",
      "/home/anton/h-elmo/abc/def\n"
     ]
    }
   ],
   "source": [
    "import helmo.experiments.test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "l = [1, 2]\n",
    "print(l and True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/', '')\n",
      "('/', 'home')\n",
      "/home\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.split('/'))\n",
    "print(os.path.split('/home'))\n",
    "print(os.path.join('/', 'home'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/', 'home', 'peganov']\n",
      "['home', 'peganov']\n",
      "['~', 'peganov']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def full_path_split(path):\n",
    "    splitted_path = list()\n",
    "    head, tail = os.path.split(path)\n",
    "    while len(head) > 0 and len(tail) > 0:\n",
    "        splitted_path = [tail] + splitted_path\n",
    "        head, tail = os.path.split(head)\n",
    "    splitted_path = [head] + splitted_path if len(head) > 0 else [tail] + splitted_path\n",
    "    return splitted_path\n",
    "\n",
    "print(full_path_split('/home/peganov'))\n",
    "print(full_path_split('home/peganov'))\n",
    "print(full_path_split('~/peganov'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = False\n",
    "b = ''\n",
    "print(a is False)\n",
    "print(b is False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "s = {'c', 'a', 'b'}\n",
    "print(sorted(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudnn_lstm_1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM as CudnnLSTM\n",
    "lstm = CudnnLSTM(1, 100)\n",
    "print(lstm.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Determinant(M):                                                 # список из списков-строк\n",
    "    if len(M[0]) == 1:\n",
    "        return(M[0])\n",
    "    elif len(M[0]) == 2:\n",
    "        return M[0][0] * M[1][1] - M[0][1]*M[1][0]    \n",
    "    else:                                                          # раскладываем по первой строке\n",
    "        \n",
    "        MatrixLength = len(M)\n",
    "        det = 0\n",
    "        for ind, el in enumerate(M[0]):                            # итерируемся по минорам\n",
    "            Minor = []\n",
    "            for i in range (1, MatrixLength):                       # итерируемся по строкам\n",
    "                MatrixString = []\n",
    "                for j in range(MatrixLength):\n",
    "                    if not (j == ind):\n",
    "                        MatrixString.append(M[i][j])  \n",
    "                Minor.append(MatrixString) \n",
    "            det += (-1)**ind * el * Determinant(Minor)\n",
    "        return (det)\n",
    "            \n",
    "print(Determinant([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(Determinant([[2, 2, 2], [1, 1, 1], [1, 2, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def Determinant(M):                                                 # список из списков-строк\n",
    "    if len(M[0]) == 1:\n",
    "        return(M[0])\n",
    "    elif len(M[0]) == 2:\n",
    "        return M[0][0] * M[1][1] - M[0][1]*M[1][0]    \n",
    "    else:                                                          # раскладываем по первой строке\n",
    "        \n",
    "        num_rows = len(M)\n",
    "        det = 0\n",
    "        for c_i, el in enumerate(M[0]):                            # итерируемся по минорам\n",
    "            submatrix = []\n",
    "            for r_i in range (1, num_rows):                       # итерируемся по строкам\n",
    "                submatrix.append(M[r_i][:c_i] + M[r_i][c_i+1:])\n",
    "            det += (-1)**c_i * el * Determinant(submatrix)\n",
    "        return (det)\n",
    "    \n",
    "print(Determinant([[2, 2, 2], [1, 1, 1], [1, 2, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 5 5 5 5 5 5 5 2\n",
      "a[i]  5\n",
      "a[i]  5\n",
      "a[i]  5\n",
      "a[i]  5\n",
      "a[i]  5\n",
      "a[i]  5\n",
      "a[i]  5\n",
      "a[i]  5\n",
      "a[i]  5\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 2] 10\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = [int(e) for e in input().split()]\n",
    "b = []\n",
    "for i in range(len(a)-1):\n",
    "    if a[i] > 2 or (a[i] == 2 and a[i+1] == 2):\n",
    "        print('a[i] ', a[i])\n",
    "        b.append(a[i])\n",
    "       \n",
    "print(a,len(a))\n",
    "print((sum(b)+a[len(a)-1])//(len(b)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
