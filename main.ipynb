{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeppavlov\n",
    "import json\n",
    "#import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from itertools import chain\n",
    "#from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.train import build_model_from_config\n",
    "from deeppavlov.core.data.dataset_reader import DatasetReader\n",
    "#from deeppavlov.core.data.utils import download_decompress\n",
    "from deeppavlov.core.common.registry import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('char_lm_dataset_reader')\n",
    "class CharLmDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Dataset reader which reads \n",
    "    \"\"\"\n",
    "    def read(self, dir_path: str, mode='self_original'):\n",
    "        dir_path = Path(dir_path)\n",
    "        dataset = {}\n",
    "        for dt in ['train', 'valid', 'test']:\n",
    "            dataset[dt] = self._parse_data(dir_path / '{}_{}.txt'.format(dt, mode))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_data(filename):\n",
    "        examples = []\n",
    "        print(filename)\n",
    "        curr_persona = []\n",
    "        curr_dialog_history = []\n",
    "        persona_done = False\n",
    "        with filename.open('r') as fin:\n",
    "            for line in fin:\n",
    "                line = ' '.join(line.strip().split(' ')[1:])\n",
    "                your_persona_pref = 'your persona: '\n",
    "                if line[:len(your_persona_pref)] == your_persona_pref and persona_done:\n",
    "                    curr_persona = [line[len(your_persona_pref):]]\n",
    "                    curr_dialog_history = []\n",
    "                    persona_done = False\n",
    "                elif line[:len(your_persona_pref)] == your_persona_pref:\n",
    "                    curr_persona.append(line[len(your_persona_pref):])\n",
    "                else:\n",
    "                    persona_done = True\n",
    "                    x, y, _, candidates = line.split('\\t')\n",
    "                    candidates = candidates.split('|')\n",
    "                    example = {\n",
    "                        'persona': curr_persona,\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                        'dialog_history': curr_dialog_history[:],\n",
    "                        'candidates': candidates,\n",
    "                        'y_idx': candidates.index(y)\n",
    "                    }\n",
    "                    curr_dialog_history.extend([x, y])\n",
    "                    examples.append(example)\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "print(len('DeepPavlov is an open-source conversational AI library built on TensorFlow and Keras.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b90e339df45c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "a = 'a' / 'b'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def greeting(name: str) -> str:\n",
    "    return name\n",
    "\n",
    "print(greeting(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(0 or 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-01 15:29:52.751 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 16: Loading dictionaries from /home/anton/dpenv/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "2018-08-01 15:29:52.805 INFO in 'pymorphy2.opencorpora_dict.wrapper'['wrapper'] at line 20: format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abc'], ['def']]\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.tokenizers.ru_tokenizer import RussianTokenizer\n",
    "\n",
    "tokenizer = RussianTokenizer(ngram_range=[1, 1])\n",
    "batch = ['abc', 'def']\n",
    "print(tokenizer(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd efgh']\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.tokenizers.utils import ngramize\n",
    "\n",
    "items = ['abcd', 'efgh']\n",
    "for i in ngramize(items, ngram_range=(2, 2)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function f at 0x7ff83c3356a8>\n",
      "__call__\n",
      "f\n",
      "Function test its fields\n",
      "('3',)\n",
      "None\n",
      "<code object f at 0x7ff83c326e40, file \"<ipython-input-17-3e15ce66597b>\", line 1>\n"
     ]
    }
   ],
   "source": [
    "def f(a, b, c='3'):\n",
    "    \"\"\"Function test its fields\"\"\"\n",
    "    d = a + b + c\n",
    "    return d\n",
    "\n",
    "print(f)\n",
    "print(f.__call__.__call__.__call__.__call__.__call__.__name__)\n",
    "print(f.__name__)\n",
    "print(f.__doc__)\n",
    "print(f.__defaults__)\n",
    "print(f.__kwdefaults__)\n",
    "print(f.__code__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function f at 0x7ff83c335e18>\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    print(f)\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you need the answer? (y/n): y\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'EssentialAnswers' has no attribute 'the_answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-22c1a5774edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEssentialAnswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthe_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPhilosopher1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthe_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPhilosopher2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEssentialAnswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'EssentialAnswers' has no attribute 'the_answer'"
     ]
    }
   ],
   "source": [
    "x = input(\"Do you need the answer? (y/n): \")\n",
    "if x.lower() == \"y\":\n",
    "    required = True\n",
    "else:\n",
    "    required = False\n",
    "    \n",
    "def the_answer(self, *args):              \n",
    "        return 42\n",
    "    \n",
    "class EssentialAnswers(type):\n",
    "    \n",
    "    def __init__(cls, clsname, superclasses, attributedict):\n",
    "        if required:\n",
    "            cls.the_answer = the_answer\n",
    "                           \n",
    "    \n",
    "class Philosopher1(metaclass=EssentialAnswers): \n",
    "    pass\n",
    "\n",
    "\n",
    "print(Philosopher1.the_answer)\n",
    "class Philosopher2(metaclass=EssentialAnswers): \n",
    "    pass\n",
    "class Philosopher3(metaclass=EssentialAnswers): \n",
    "    pass\n",
    "    \n",
    "    \n",
    "plato = Philosopher1()\n",
    "print(plato.the_answer())\n",
    "kant = Philosopher2()\n",
    "# let's see what Kant has to say :-)\n",
    "print(kant.the_answer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-03 15:36:34.385 INFO in 'char_lm_vocab'['char_lm_vocab'] at line 70: [loading vocabulary from /home/anton/DeepPavlov/download/vocab.dict]\n",
      "2018-08-03 15:36:34.400 INFO in 'char_lm_vocab'['char_lm_vocab'] at line 70: [loading vocabulary from /home/anton/DeepPavlov/download/vocab.dict]\n",
      "2018-08-03 15:36:34.412 INFO in 'char_lm_vocab'['char_lm_vocab'] at line 59: [saving vocabulary to /home/anton/DeepPavlov/download/vocab.dict]\n"
     ]
    }
   ],
   "source": [
    "from char_lm_vocab import CharLMVocabulary\n",
    "vocab = CharLMVocabulary(\n",
    "    save_path='./vocab.dict',\n",
    "    load_path='./vocab.dict',\n",
    ")\n",
    "text = 'DeepPavlov is an open-source conversational AI library built on TensorFlow and Keras.'\n",
    "vocab.fit(text)\n",
    "vocab.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "r\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "[3, 14, None]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(vocab[5])\n",
    "print('z' in vocab)\n",
    "print('a' in vocab)\n",
    "print(vocab.is_str_batch([]))\n",
    "print(vocab.is_str_batch(['c']))\n",
    "print(vocab(['a', 'b', 'я']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99608877cb86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCudnnLSTM\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Bring in subpackages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeneratorEnqueuer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedEnqueuer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, tp)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Invokes __set__.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m_import_module\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m\"\"\"Import module, returning the module after the last dot.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM as LSTM\n",
    "import numpy as np\n",
    "\n",
    "num_units = 100\n",
    "voc_size = 20\n",
    "inps = tf.placeholder(tf.int32, shape=[None, None])\n",
    "lbls = tf.placeholder(tf.int32, shape=[None, None])\n",
    "prep_inps = tf.one_hot(inps, voc_size)\n",
    "prep_lbls = tf.one_hot(lbls, voc_size)\n",
    "\n",
    "out_w = tf.Variable(\n",
    "    tf.truncated_normal([num_units, voc_size])\n",
    ")\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    lstm = LSTM(\n",
    "        2,\n",
    "        num_units,\n",
    "        input_mode='skip_input',\n",
    "    )\n",
    "\n",
    "bs = tf.shape(inps)[1:-1]\n",
    "state_shape = tf.concat([[2], bs, [num_units]], 0)\n",
    "with tf.device('/gpu:0'):\n",
    "    lstm_res, state = lstm(\n",
    "        prep_inps\n",
    "    )\n",
    "    print(lstm_res)\n",
    "    logits = tf.matmul(tf.reshape(lstm_res, [-1, num_units]), out_w)\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits,\n",
    "        labels=tf.reshape(prep_lbls, [-1, voc_size])\n",
    "    ))\n",
    "    opt = tf.train.GradientDescentOptimizer(1.)\n",
    "    train_op = opt.minimize(loss)\n",
    "    \n",
    "nu = 10\n",
    "bs = 32\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(100):\n",
    "        inputs = np.random.randint(0, voc_size, size=(nu, bs))\n",
    "        labels = np.random.randint(0, voc_size, size=(nu, bs))\n",
    "        _, preds = sess.run([train_op, preds], feed_dict={inps: inputs, lbls: labels})\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def create_distribute_map(num_distributed, result_length):\n",
    "    quotient = result_length / num_distributed\n",
    "    num_filled = 0\n",
    "    if num_distributed < result_length:\n",
    "        num_repeats = list()\n",
    "        for i in range(1, num_distributed):\n",
    "            filled_space = quotient * i\n",
    "            print(filled_space)\n",
    "            truncated = int(filled_space)\n",
    "            num_repeats.append(truncated - num_filled)\n",
    "            num_filled = truncated\n",
    "        num_repeats.append(result_length - num_filled)\n",
    "    else:\n",
    "        num_repeats = [1] * result_length\n",
    "    map_ = list(itertools.chain(*[[i] * n_rep for i, n_rep in enumerate(num_repeats)]))\n",
    "    return map_\n",
    "\n",
    "print(create_distribute_map(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10, -1, -1):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9dc84027a5a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with open('debug.txt', 'r') as f:\n",
    "    t = f.read()\n",
    "print(t.split('\\n')[0])\n",
    "print('\"%s\"' % t)\n",
    "l = []\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "from useful_functions import all_combs\n",
    "\n",
    "def create_even_distribution(num_unique_items, num_slots):\n",
    "    \"\"\"Returns a list with number of slots taken by each item. If num_slots < num_unique_items not all items\n",
    "    will be distributed\"\"\"\n",
    "    items_per_slot_float = num_slots / num_unique_items\n",
    "    total_slots_filled_saved = 0\n",
    "    if num_unique_items < num_slots:\n",
    "        item_distribution = list()\n",
    "        for i in range(1, num_unique_items):\n",
    "            total_slots_float = items_per_slot_float * i\n",
    "            total_slots_filled = int(total_slots_float)\n",
    "            item_distribution.append(total_slots_filled - total_slots_filled_saved)\n",
    "            total_slots_filled_saved = total_slots_filled\n",
    "        item_distribution.append(num_slots - total_slots_filled_saved)\n",
    "    else:\n",
    "        item_distribution = [1] * num_slots\n",
    "    return item_distribution\n",
    "\n",
    "\n",
    "def create_distribute_map(num_unique_items, num_slots):\n",
    "    item_distribution = create_even_distribution(num_unique_items, num_slots)\n",
    "    map_ = list(itertools.chain(*[[i] * n_rep for i, n_rep in enumerate(item_distribution)]))\n",
    "    return map_\n",
    "\n",
    "\n",
    "def approx_mem_consumption(config):\n",
    "    # in MB for sequence_length=100, batch_size=128, num_layers=2, num_units=2000\n",
    "    # working for all lstm variants\n",
    "    base_consumption = 4500\n",
    "    consumption = base_consumption * \\\n",
    "        (config['num_units'] / 2000)**2 * \\\n",
    "        (config['num_layers'] / 2) * \\\n",
    "        (config['batch_size'] / 128) * \\\n",
    "        (config['sequence_length'] / 100)\n",
    "    return consumption\n",
    "\n",
    "\n",
    "def num_consequent_repeats(list_, idx):\n",
    "    current = list_[idx]\n",
    "    i = 1\n",
    "    while list_[idx+i] == current:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def ceil(a):\n",
    "    if int(a) != a:\n",
    "        return int(a) + 1\n",
    "    return int(a)\n",
    "\n",
    "\n",
    "def split_experiment_config_into_separate_measurement_configs(config):\n",
    "    seq_lens = make_list(config['sequence_length'])\n",
    "    batch_sizes = make_list(config['batch_size'])\n",
    "    num_layers = make_list(config['num_layers'])\n",
    "    num_units = make_list(config['num_units'])\n",
    "    combs = all_combs([seq_lens, batch_sizes, num_layers, num_units])\n",
    "    configs = list()\n",
    "    for comb in combs:\n",
    "        conf = copy.deepcopy(config)\n",
    "        conf['sequence_length'] = comb[0]\n",
    "        conf['batch_size'] = comb[1]\n",
    "        conf['num_layers'] = comb[2]\n",
    "        conf['num_units'] = comb[3]\n",
    "        configs.append(conf)\n",
    "    if config['num_repeats'] == 1:\n",
    "        return configs\n",
    "    else:\n",
    "        confs = list()\n",
    "        for conf in configs:\n",
    "            confs.extend([conf]*config['num_repeats'])\n",
    "        return confs\n",
    "\n",
    "\n",
    "def get_configs_run_in_parallel(configs, counter):\n",
    "    current_config = configs[counter]\n",
    "    max_num_in_parallel = current_config['memory_per_experiment'] / approx_mem_consumption(current_config)\n",
    "    num_repeats = num_consequent_repeats(configs, counter)\n",
    "    configs_to_process = configs[counter:counter+num_repeats]\n",
    "    num_launches_required = ceil(num_repeats / max_num_in_parallel)\n",
    "    distribution = create_even_distribution(num_launches_required, num_repeats)\n",
    "    list_of_launches = list()\n",
    "    pointer = 0\n",
    "    for num_proc in distribution:\n",
    "        list_of_launches.append(configs_to_process[pointer:pointer+num_proc])\n",
    "        pointer += num_proc\n",
    "    return list_of_launches, num_repeats\n",
    "\n",
    "\n",
    "def make_list(candidate):\n",
    "    if isinstance(candidate, list):\n",
    "        l = candidate\n",
    "    else:\n",
    "        l = [candidate]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"num_layers\": [3, 4],\n",
    "  \"num_units\": [1000, 2000],\n",
    "  \"sequence_length\": [50, 100],\n",
    "  \"batch_size\": [128],\n",
    "  \"save_path\": \"results/lstm_test\",\n",
    "  \"lstm_type\": \"cudnn_stacked\",\n",
    "  \"mode\": \"train\",\n",
    "  \"num_steps\": 500,\n",
    "  \"num_proc_in_pool\": 5,\n",
    "  \"num_repeats\": 10,\n",
    "  \"memory_per_experiment\": 7500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "\n",
      "\n",
      "1\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "{'num_units': 1000, 'num_steps': 500, 'num_layers': 3, 'memory_per_experiment': 7500, 'mode': 'train', 'sequence_length': 50, 'save_path': 'results/lstm_test', 'batch_size': 128, 'num_proc_in_pool': 5, 'lstm_type': 'cudnn_stacked', 'num_repeats': 10}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "configs = split_experiment_config_into_separate_measurement_configs(config)\n",
    "launches = get_configs_run_in_parallel(configs, 0)\n",
    "for idx, launch in enumerate(launches[0]):\n",
    "    print(idx)\n",
    "    for conf in launch:\n",
    "        print(conf)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843.75\n"
     ]
    }
   ],
   "source": [
    "print(approx_mem_consumption(configs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
