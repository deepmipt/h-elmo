{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_relevant_activations(data, markup_for_tag):\n",
    "    result = []\n",
    "    for i, tag in enumerate(markup_for_tag):\n",
    "        if tag != 0:\n",
    "            result.append(data[i])\n",
    "    return np.stack(result)\n",
    "\n",
    "\n",
    "def get_matches(activations, markup):\n",
    "    markup = np.array(markup)\n",
    "    markup_devs = markup - np.mean(markup)\n",
    "    activation_devs = activations - np.mean(activations, 0, keepdims=True)\n",
    "    activation_stddevs = np.std(activations, 0, ddof=1, keepdims=True)\n",
    "    markup_stddev = np.std(markup, ddof=1)\n",
    "    activation_dev_fractions = activation_devs / (activation_stddevs + 1e-20)\n",
    "    markup_dev_fractions = markup_devs / (markup_stddev + 1e-20)\n",
    "    return activation_dev_fractions * np.reshape(markup_dev_fractions, [-1, 1])\n",
    "\n",
    "\n",
    "def compute_stats(data, markup_for_tag):\n",
    "    markup_for_tag = np.array(markup_for_tag)\n",
    "    stats = {}\n",
    "    stats['markup'] = markup_for_tag\n",
    "    stats['relevant_markup'] = list(filter(lambda x: x != 0, markup_for_tag))\n",
    "    stats['relevant_activations'] = get_relevant_activations(data, markup_for_tag)\n",
    "    stats['matches'] = get_matches(stats['relevant_activations'], stats['relevant_markup'])\n",
    "    stats['correlations'] = np.mean(stats['matches'], 0)\n",
    "    assert stats['correlations'].ndim == 1\n",
    "    stats['match_stddevs'] = np.std(stats['matches'], 0)\n",
    "    stats['mean_square_correlation'] = np.sqrt(np.mean(stats['correlations']**2))\n",
    "    stats['meta'] = {\n",
    "        \"positive\": np.count_nonzero(markup_for_tag == 1),\n",
    "        \"negative\": np.count_nonzero(markup_for_tag == -1),\n",
    "        \"total\": len(stats['markup']),\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('test', exist_ok=True)\n",
    "\n",
    "with open(\"test/test.pickle\", 'wb') as f:\n",
    "    pickle.dump(np.array([[4]*10]*50), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_axis_quarters(tensor):\n",
    "    last_dim = tf.shape(tensor, out_type=tf.float32)[-1]\n",
    "    exponents = tf.range(0., last_dim, 1., dtype=tf.float32)\n",
    "    powers = tf.math.pow(2., exponents)\n",
    "    binary_format = tf.cast(tensor > 0, tf.float32)\n",
    "    linear_combination = powers * binary_format\n",
    "    numbers = tf.reduce_sum(linear_combination, axis=-1)\n",
    "    return tf.cast(numbers, tf.int32)\n",
    "\n",
    "tensor = tf.constant(\n",
    "    [[1, -1, 1],\n",
    "     [-1, -1, -1]]\n",
    ")\n",
    "\n",
    "axis_quarters = get_axis_quarters(tensor)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(axis_quarters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "prefix = '/media/anton/DATA/results/h-elmo/expres/resrnn/poscorr/4/9/corr/level1_1/NNS'\n",
    "\n",
    "tmpl = os.path.join(prefix, '{}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = tmpl.format('correlations')\n",
    "with open(file_name, 'rb') as f:\n",
    "    corr = pickle.load(f)\n",
    "    \n",
    "print(max(corr))\n",
    "print(np.argmax(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "\n",
    "c = Counter(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = tmpl.format('matches')\n",
    "with open(matches, 'rb') as f:\n",
    "    m = pickle.load(f)\n",
    "\n",
    "m62 = m[:, 62]\n",
    "print(m62)\n",
    "print(max(m62))\n",
    "print(np.argmax(m62))\n",
    "print(min(m62))\n",
    "print(np.argmin(m62))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(m62))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "f1 = m62[m62 > threshold]\n",
    "f2 = m62[m62 <= -threshold]\n",
    "filtered = np.concatenate([f1, f2])\n",
    "plt.hist(filtered, bins=100, density=True)\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('matches')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = tmpl.format('relevant_activations')\n",
    "with open(act, 'rb') as f:\n",
    "    act = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = tmpl.format('relevant_markup')\n",
    "with open(markup, 'rb') as f:\n",
    "    markup = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act62 = act[:, 62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(act62, bins=100, density=True)\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('activations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helmo.util.plot.plot_helpers import density_plot\n",
    "\n",
    "density_plot(m62, 0.001, None, 'blue')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('matches')\n",
    "plt.ylabel('density')\n",
    "plt.grid()\n",
    "plt.savefig(\n",
    "    '/media/anton/DATA/results/h-elmo/expres/resrnn/poscorr/4/9/corr/level1_1/NNS/plots/matches.png',\n",
    "    dpi=900\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helmo.util.plot.plot_helpers import density_plot\n",
    "\n",
    "density_plot(act62, 0.0001, None, 'blue')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('activations')\n",
    "plt.ylabel('density')\n",
    "plt.grid()\n",
    "plt.savefig(\n",
    "    '/media/anton/DATA/results/h-elmo/expres/resrnn/poscorr/4/9/corr/level1_1/NNS/plots/activations.png',\n",
    "    dpi=900\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helmo.util.plot.plot_helpers import density_plot\n",
    "\n",
    "selected_indices = np.array(np.array(markup) + 1, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns_act62 = act62[selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(nns_act62.shape)\n",
    "print(nns_act62[:100])\n",
    "min_ = np.min(act62)\n",
    "max_ = np.max(act62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "density_plot(act62, 0.0001, 'all activations', 'blue')\n",
    "density_plot(nns_act62, 0.0001, 'NNS activations', 'red', [min_, max_])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('activations')\n",
    "plt.ylabel('density')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\n",
    "    os.path.join(prefix, 'plots/activations_and_nns_activations.png'),\n",
    "    dpi=900\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.01*i for i in range(-200, 200)]\n",
    "y = [1 / np.log(abs(xx-1.)) for xx in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = m[m > 1.]\n",
    "f2 = m[m < -1.]\n",
    "filtered = np.concatenate([f1, f2])\n",
    "plt.hist(filtered, bins=1000, density=True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpl = '/media/anton/DATA/results/h-elmo/expres/resrnn/poscorr/4/9/corr/level0_0/NNS/{}.pickle'\n",
    "matches = tmpl.format('matches')\n",
    "with open(matches, 'rb') as f:\n",
    "    m = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = tmpl.format('activations')\n",
    "with open(act, 'rb') as f:\n",
    "    a = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddevs = tmpl.format('match_stddevs')\n",
    "with open(stddevs, 'rb') as f:\n",
    "    std = pickle.load(f)\n",
    "\n",
    "print(std)\n",
    "print(max(std))\n",
    "print(np.argmax(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_all_values_except_specified(tensor, excluded):\n",
    "    with tf.name_scope('get_all_values_except_specified'):\n",
    "        tensor = tf.reshape(tensor, [-1])\n",
    "        excluded = tf.reshape(excluded, [-1])\n",
    "        excluded_shape = tf.shape(excluded)\n",
    "        tensor_expanded = tf.reshape(tensor, [-1, 1])\n",
    "        multiples = tf.concat([[1], excluded_shape], 0)\n",
    "        tensor_expanded = tf.tile(tensor_expanded, multiples)\n",
    "        masks = tf.cast(tf.equal(tf.cast(tensor_expanded, tf.int32), tf.cast(excluded, tf.int32)), tf.int32)\n",
    "        mask = tf.reduce_sum(masks, [1])\n",
    "        mask = tf.cast(tf.cast(mask, dtype=tf.bool), dtype=tf.int32) - 1\n",
    "        return tf.boolean_mask(tensor, mask)\n",
    "    \n",
    "\n",
    "tensor, num_dims, axes, output = (\n",
    "                    [[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n",
    "\n",
    "                    5,\n",
    "\n",
    "                    [0, 2, 4],\n",
    "\n",
    "                    [[[[[1, 2]], [[3, 4]]], [[[5, 6]], [[7, 8]]]]],\n",
    "                )\n",
    "\n",
    "if not tf.contrib.framework.is_tensor(tensor):\n",
    "    tensor = tf.constant(tensor)\n",
    "if not tf.contrib.framework.is_tensor(axes):\n",
    "    axes = tf.constant(axes, dtype=tf.int32)\n",
    "sh = tf.shape(tensor, out_type=tf.int32)\n",
    "nd = tf.shape(sh, out_type=tf.int32)[0]\n",
    "assert_axes_smaller_than_num_dims = tf.assert_less(\n",
    "    axes, num_dims, message='`axes` has to be less than `num_dims`')\n",
    "check_num_dims = tf.assert_greater_equal(\n",
    "    num_dims, nd,\n",
    "    message='`num_dims` has to be greater or equal to number of dimensions in `tensor`'\n",
    ")\n",
    "ass_axes_bigger_or_equal_than_num_dims = tf.assert_greater_equal(axes, -num_dims)\n",
    "\n",
    "negative_axes_mask = tf.cast(axes < 0, tf.int32)\n",
    "axes += negative_axes_mask * num_dims\n",
    "\n",
    "ones_for_expansion = tf.ones(tf.reshape(num_dims - nd, [1]), dtype=tf.int32)\n",
    "shape_for_expansion = tf.concat([sh, ones_for_expansion], 0)\n",
    "\n",
    "tensor = tf.reshape(tensor, shape_for_expansion)\n",
    "\n",
    "# remained_axes = get_all_values_except_specified(tf.range(num_dims, dtype=tf.int32), axes)\n",
    "# perm = tf.concat([axes, remained_axes], 0)\n",
    "updates = tf.range(0, num_dims, 1, dtype=tf.int32)\n",
    "remained_positions = get_all_values_except_specified(tf.range(num_dims, dtype=tf.int32), axes)\n",
    "indices = tf.concat([axes, remained_positions], 0)\n",
    "indices = tf.reshape(indices, [-1, 1])\n",
    "perm_shape = tf.reshape(num_dims, [1])\n",
    "perm = tf.scatter_nd(indices, updates, perm_shape)\n",
    "\n",
    "with tf.control_dependencies([check_num_dims, assert_axes_smaller_than_num_dims, ass_axes_bigger_or_equal_than_num_dims]):\n",
    "    tensor = tf.transpose(tensor, perm=perm)\n",
    "    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from helmo.util.scripts.text_neuron_correlation import compute_stats\n",
    "\n",
    "num_unrollings = 20\n",
    "num_units = 4\n",
    "\n",
    "# a = np.random.rand(num_units, num_unrollings)\n",
    "\n",
    "m = np.random.choice([-1, 1, 0], num_unrollings)\n",
    "a = np.stack([m]*num_units, 1)\n",
    "print(a)\n",
    "\n",
    "stats = compute_stats(a, m)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_int_part(n):\n",
    "    return str(int(n // 1))\n",
    "\n",
    "\n",
    "def get_frac_part(n):\n",
    "    removed = int(get_int_part(n))\n",
    "    frac_part = ''\n",
    "    while n % 1:\n",
    "        n *= 10\n",
    "        removed *= 10\n",
    "        frac_part += str(int(n // 1) - removed)\n",
    "        removed = int(n // 1)\n",
    "    return frac_part\n",
    "\n",
    "\n",
    "def get_kth_digit(number, k, default='0'):\n",
    "    \"\"\"Returns k-th digit. For example, in number 123.45 1 \n",
    "    is 2nd digit, 3 is zeroth and 5 is -2nd. \n",
    "    If the number does not have such a digit default is returned.\n",
    "    Args:\n",
    "        number: float or str convertable to float\n",
    "        k: integer\n",
    "    Returns:\n",
    "        str\"\"\"\n",
    "    if isinstance(number, str):\n",
    "        number = float(number)\n",
    "    int_part = get_int_part(number)\n",
    "    frac_part = get_frac_part(number)\n",
    "    number = int_part + frac_part\n",
    "    k = len(int_part) - k - 1\n",
    "    if 0 <= k < len(number):\n",
    "        return number[k]\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "    \n",
    "def get_first_nonzero_digit_pos(n):\n",
    "    if n == 0:\n",
    "        return None\n",
    "    int_part = get_int_part(n)\n",
    "    frac_part = get_frac_part(n)\n",
    "    if int(int_part):\n",
    "        return len(int_part) - 1\n",
    "    i = 0\n",
    "    while i < len(frac_part) and not int(frac_part[i]):\n",
    "        i += 1\n",
    "    assert frac_part[i] != '0'\n",
    "    return -i - 1\n",
    "\n",
    "\n",
    "def get_acc_num_digits(std, acc):\n",
    "    if std == 0:\n",
    "        return None\n",
    "    \n",
    "    std_err = std * acc\n",
    "    \n",
    "    nz_err = get_first_nonzero_digit_pos(std_err)\n",
    "    \n",
    "    digit_1_pos_higher = get_kth_digit(std, nz_err+1)\n",
    "    \n",
    "    higher_digit_change = get_kth_digit(std + std_err, nz_err+1) != digit_1_pos_higher or \\\n",
    "        get_kth_digit(std - std_err, nz_err+1) != digit_1_pos_higher\n",
    "\n",
    "    if higher_digit_change:\n",
    "        nz_err += 1\n",
    "    return nz_err\n",
    "\n",
    "for std in np.linspace(0, 1, 101):\n",
    "    last_digit = get_acc_num_digits(std, 0.2)\n",
    "    print(std, last_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_kth_digit(123.45678, -4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_nonzero_digit_pos_for_std(std, acc):\n",
    "    if std == 0:\n",
    "        return None\n",
    "    std_fraction = std * acc\n",
    "    int_part = get_int_part(std_fraction)\n",
    "    frac_part = get_frac_part(std_fraction)\n",
    "    if int(int_part):\n",
    "        return len(int_part) - 1\n",
    "    i = 0\n",
    "    while i < len(frac_part) and not int(frac_part[i]):\n",
    "        i += 1\n",
    "    assert frac_part[i] != '0'\n",
    "    return -i - 1\n",
    "\n",
    "\n",
    "get_first_nonzero_digit_pos_for_std(0.0123456, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_part(n):\n",
    "    return str(int(n // 1))\n",
    "\n",
    "\n",
    "def get_frac_part(n):\n",
    "    removed = int(get_int_part(n))\n",
    "    frac_part = ''\n",
    "    while n % 1:\n",
    "        n *= 10\n",
    "        removed *= 10\n",
    "        frac_part += str(int(n // 1) - removed)\n",
    "        removed = int(n // 1)\n",
    "    return frac_part\n",
    "\n",
    "len(get_frac_part(2.2250738585072014e-308))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bins = np.histogram_bin_edges([1], 8, [1., 9.])\n",
    "a = np.array([-1., 0., 1., 1.5, 2.5, 10.4])\n",
    "d = np.digitize(a, bins)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "a = np.zeros([10**4, 10**4])\n",
    "b = np.zeros([10**4, 10**4])\n",
    "\n",
    "N = 100\n",
    "\n",
    "t = timeit.timeit(\n",
    "    stmt=\"c = a + b\",\n",
    "    globals=dict(a=a, b=b),\n",
    "    number=N\n",
    ")\n",
    "\n",
    "print(t / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.Variable(0, trainable=False)\n",
    "\n",
    "op = tf.assign_add(a, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run([op, op])\n",
    "    print(a.eval(sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = net.get_default_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(net._hooks.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h['update_level0_0_hidden_state_hist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.group()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "\n",
    "k, n, p = symbols('k n p')\n",
    "\n",
    "p = Product(k**(k * binomial(n, k) * p**k * (1-p)**(n-k)), (k, 0, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = p.doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_line(file_name, start_idx):\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f.readlines()[start_idx:]:\n",
    "            xx, yy = line.split()\n",
    "            x.append(float(xx))\n",
    "            y.append(float(yy))\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def load_lines(file_names, start_idx):\n",
    "    lines = []\n",
    "    for fn in file_names:\n",
    "        lines.append(load_line(fn, start_idx))\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_groups_of_lines(groups):\n",
    "    lines = {}\n",
    "    for label, specs in groups.items():\n",
    "        lines[label] = load_lines(specs['names'], specs['start_idx'])\n",
    "    return lines\n",
    "\n",
    "\n",
    "def plot_similar_lines(lines, color, lw):\n",
    "    for line in lines:\n",
    "        plt.plot(line[0], line[1], lw=lw, color=color)\n",
    "        \n",
    "        \n",
    "def add_legend(artists, labels, position):\n",
    "    if position == 'outside':\n",
    "        pos_dict = dict(\n",
    "            bbox_to_anchor=(1.05, 1),\n",
    "            loc=2,\n",
    "        )\n",
    "    elif position == 'upper_right':\n",
    "        pos_dict = dict(\n",
    "            bbox_to_anchor=(.95, .95),\n",
    "            loc=1,\n",
    "        )\n",
    "    elif position == 'upper_left':\n",
    "        pos_dict = dict(\n",
    "            bbox_to_anchor=(.05, .95),\n",
    "            loc=2,\n",
    "        )\n",
    "    elif position == 'best':\n",
    "        pos_dict = {'loc': 'best'}\n",
    "    ax = plt.gca()\n",
    "    lgd = ax.legend(\n",
    "        artists,\n",
    "        labels,\n",
    "        **pos_dict,\n",
    "    )\n",
    "    return lgd\n",
    "\n",
    "\n",
    "def form_symlog_kwargs(groups):\n",
    "    x_nonzero_values = []\n",
    "    y_nonzero_values = []\n",
    "    for group_of_lines in groups.values():\n",
    "        for line in group_of_lines:\n",
    "            x_nonzero_values += [x for x in line[0] if x != 0]\n",
    "            y_nonzero_values += [y for y in line[1] if y != 0]\n",
    "    xkwargs = dict(\n",
    "        linthreshx=np.min(np.abs(x_nonzero_values))\n",
    "    )\n",
    "    ykwargs = dict(\n",
    "        linthreshy=np.min(np.abs(y_nonzero_values))\n",
    "    )\n",
    "    return xkwargs, ykwargs\n",
    "\n",
    "\n",
    "def plot_groups_of_lines(\n",
    "        groups,\n",
    "        single_lines,\n",
    "        group_colors,\n",
    "        single_colors,\n",
    "        lw,\n",
    "        xlabel,\n",
    "        ylabel,\n",
    "        xscale,\n",
    "        yscale,\n",
    "        xaxis_format,\n",
    "        start_idx,\n",
    "        legend_position,\n",
    "        dpi,\n",
    "        save_path,\n",
    "        show,\n",
    "):\n",
    "    custom_lines = []\n",
    "    labels = []\n",
    "    for (label, group_of_lines), color in zip(groups.items(), group_colors):\n",
    "        labels.append(label)\n",
    "        custom_lines.append(Line2D([0], [0], color=color, lw=4))\n",
    "        plot_similar_lines(group_of_lines, color, lw)\n",
    "    for (label, line), color in zip(single_lines.items(), single_colors):\n",
    "        labels.append(label)\n",
    "        custom_lines.append(Line2D([0], [0], color=color, lw=4))\n",
    "        plt.plot(line[0][start_idx:], line[1][start_idx:], color=color, lw=lw)\n",
    "    plt.grid(which='both')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    xkwargs, ykwargs = form_symlog_kwargs(groups)\n",
    "    if xscale != 'symlog':\n",
    "        xkwargs = {}    \n",
    "    if yscale != 'symlog':\n",
    "        ykwargs = {}\n",
    "    plt.xscale(xscale, **xkwargs)\n",
    "    plt.yscale(yscale, **ykwargs)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(xaxis_format)\n",
    "    bbox_extra_artists = [add_legend(custom_lines, labels, legend_position)]\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.split(save_path)[0], exist_ok=True)\n",
    "        plt.savefig(\n",
    "            save_path,\n",
    "            bbox_extra_artists=bbox_extra_artists,\n",
    "            bbox_inches='tight',\n",
    "            dpi=dpi,\n",
    "        )\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def load_single_lines(file_names):\n",
    "    lines = {}\n",
    "    for label, fn in file_names.items():\n",
    "        lines[label] = load_line(fn, 0)\n",
    "    return lines\n",
    "        \n",
    "        \n",
    "def main(\n",
    "        groups_of_file_names,\n",
    "        file_names,\n",
    "        group_colors,\n",
    "        single_colors,\n",
    "        lw,\n",
    "        xlabel,\n",
    "        ylabel,\n",
    "        xscale,\n",
    "        yscale,\n",
    "        xaxis_format,\n",
    "        start_idx,\n",
    "        legend_position,\n",
    "        dpi,\n",
    "        save_path,\n",
    "        show,\n",
    "):\n",
    "    groups = load_groups_of_lines(groups_of_file_names)\n",
    "    single_lines = load_single_lines(file_names)\n",
    "    plot_groups_of_lines(\n",
    "        groups,\n",
    "        single_lines,\n",
    "        group_colors,\n",
    "        single_colors,\n",
    "        lw,\n",
    "        xlabel,\n",
    "        ylabel,\n",
    "        xscale,\n",
    "        yscale,\n",
    "        xaxis_format,\n",
    "        start_idx,\n",
    "        legend_position,\n",
    "        dpi,\n",
    "        save_path,\n",
    "        show,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "prefix = 'expres/resrnn/word/restore_tt'\n",
    "prefix2 = 'expres/resrnn/two_branches/tt'\n",
    "postfix = 'results_shifted/loss_valid.txt'\n",
    "postfix2 = 'results/loss_valid.txt'\n",
    "\n",
    "nets = ['ch100_w200', 'ch100_w100', 'ch100_w50']\n",
    "numbers = list(range(10))\n",
    "\n",
    "groups_of_file_names = OrderedDict()\n",
    "for net in nets:\n",
    "    name = net.replace('_', ' + ') + ' posttraining'\n",
    "    groups_of_file_names[name] = {'names': [], 'start_idx': 0}\n",
    "    for i in numbers:\n",
    "        file_name = os.path.join(prefix, net, str(i), postfix)\n",
    "        groups_of_file_names[name]['names'].append(file_name)\n",
    "for net in nets:\n",
    "    name = net.replace('_', ' + ')\n",
    "    groups_of_file_names[name] = {'names': [], 'start_idx': 200}\n",
    "    for i in numbers:\n",
    "        file_name = os.path.join(prefix2, net, str(i), postfix2)\n",
    "        groups_of_file_names[name]['names'].append(file_name)\n",
    "        \n",
    "print(list(groups_of_file_names.keys()))\n",
    "\n",
    "pre_x, pre_y = load_line(os.path.join(prefix, 'loss_pretrain.txt'), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(groups_of_file_names.keys())\n",
    "\n",
    "main(\n",
    "    groups_of_file_names,\n",
    "    {'pretraining ch100': os.path.join(prefix, 'loss_pretrain.txt')},\n",
    "    ['red', 'green', 'blue', 'pink', 'lime', 'cyan'],\n",
    "    ['black'],\n",
    "    0.2,\n",
    "    'step',\n",
    "    'loss',\n",
    "    'linear',\n",
    "    'linear',\n",
    "    mpl.ticker.EngFormatter(),\n",
    "    200,\n",
    "    'outside',\n",
    "    900,\n",
    "    os.path.join(prefix, 'plots/pre_and_post_training.png'),\n",
    "    True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    values = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                values.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    return values\n",
    "\n",
    "\n",
    "prefix = '/media/anton/DATA/results/h-elmo/expres/entropy/first_experiment/hist'\n",
    "path = 'tensors/valid/accumulator_postprocessing'\n",
    "dirs = [str(i) for i in range(3)]\n",
    "layers = ['level0_0', 'level0_1']\n",
    "labels = OrderedDict([('level0_0', 'char encoder'), ('level0_1', 'char decoder')])\n",
    "\n",
    "entropy_tmpl = 'mean_entropy_{}_hidden_state.pickle'\n",
    "mi_tmpl = 'mean_mi_{}_hidden_state.pickle'\n",
    "\n",
    "path_to_loss = os.path.join(prefix, '0/results/loss_valid.txt')\n",
    "\n",
    "def get_steps(fn):\n",
    "    steps = []\n",
    "    with open(fn) as f:\n",
    "        for line in f.readlines():\n",
    "            step = int(line.split()[0])\n",
    "            steps.append(step)\n",
    "    return steps\n",
    "\n",
    "steps = get_steps(path_to_loss)\n",
    "        \n",
    "entropy_lines = OrderedDict(zip(labels.values(), [[] for _ in layers]))\n",
    "mi_lines = OrderedDict(zip(labels.values(), [[] for _ in layers]))\n",
    "\n",
    "for d in dirs:\n",
    "    for layer in layers:\n",
    "        entropy_fn = entropy_tmpl.format(layer)\n",
    "        entropy_fn = os.path.join(prefix, d, path, entropy_fn)\n",
    "        mi_fn = mi_tmpl.format(layer)\n",
    "        mi_fn = os.path.join(prefix, d, path, mi_fn)\n",
    "        label = labels[layer]\n",
    "        entropy_lines[label].append([steps, load_pickle(entropy_fn)])\n",
    "        mi_lines[label].append([steps, load_pickle(mi_fn)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(prefix, 'plots/entropy.png')\n",
    "\n",
    "plot_groups_of_lines(\n",
    "    entropy_lines,\n",
    "    {},\n",
    "    ['red', 'blue'],\n",
    "    [],\n",
    "    0.4,\n",
    "    'step',\n",
    "    'bits',\n",
    "    'log',\n",
    "    'linear',\n",
    "    mpl.ticker.ScalarFormatter(),\n",
    "    0,\n",
    "    'best',\n",
    "    900,\n",
    "    save_path,\n",
    "    True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(prefix, 'plots/mi.png')\n",
    "\n",
    "plot_groups_of_lines(\n",
    "    mi_lines,\n",
    "    {},\n",
    "    ['red', 'blue'],\n",
    "    [],\n",
    "    0.4,\n",
    "    'step',\n",
    "    'bits',\n",
    "    'log',\n",
    "    'linear',\n",
    "    mpl.ticker.ScalarFormatter(),\n",
    "    0,\n",
    "    'best',\n",
    "    900,\n",
    "    save_path,\n",
    "    True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    values = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                values.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    return values\n",
    "\n",
    "\n",
    "prefix = '/media/anton/DATA/results/h-elmo/expres/correlation/nocorrloss/wide'\n",
    "path = 'tensors/valid/pickle_mean_tensors/mean_sqr_correlation.pickle'\n",
    "dirs = [str(i) for i in range(3)]\n",
    "dropout = ['0', '0.2', '0.4', '0.7']\n",
    "labels = OrderedDict([('0', 'dropout 0'), ('0.2', 'dropout 0.2'), ('0.4', 'dropout 0.4'), ('0.7', 'dropout 0.7')])\n",
    "\n",
    "path_to_loss = os.path.join(prefix, '0/0/results/loss_valid.txt')\n",
    "\n",
    "def get_steps(fn):\n",
    "    steps = []\n",
    "    with open(fn) as f:\n",
    "        for line in f.readlines():\n",
    "            step = int(line.split()[0])\n",
    "            steps.append(step)\n",
    "    return steps\n",
    "\n",
    "steps = get_steps(path_to_loss)\n",
    "print(steps)\n",
    "        \n",
    "correlation_lines = OrderedDict(zip(labels.values(), [[] for _ in dropout]))\n",
    "\n",
    "for d in dirs:\n",
    "    for dp in dropout:\n",
    "        correlation_fn = os.path.join(prefix, dp, d, path)\n",
    "        label = labels[dp]\n",
    "        y = load_pickle(correlation_fn)[0]\n",
    "        print(y.shape)\n",
    "        correlation_lines[label].append([steps, y])\n",
    "a = correlation_lines['dropout 0.4'][0][1]\n",
    "# correlation_lines['dropout 0.4'][0][1] = np.concatenate([a[:64], a[65:]])\n",
    "# m = float('inf')\n",
    "# for i in range(100):\n",
    "#     if abs(a[i] - a[i+1]) < m:\n",
    "#         m = abs(a[i] - a[i+1])\n",
    "#         j = i\n",
    "# print(j, abs(a[j] - a[j+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(prefix, 'plots/mean_sqr_correlation_bundles.png')\n",
    "\n",
    "plot_groups_of_lines(\n",
    "    correlation_lines,\n",
    "    {},\n",
    "    ['red', 'blue', 'black', 'brown'],\n",
    "    [],\n",
    "    0.4,\n",
    "    'step',\n",
    "    'mean square correlation',\n",
    "    'log',\n",
    "    'linear',\n",
    "    mpl.ticker.ScalarFormatter(),\n",
    "    0,\n",
    "    'best',\n",
    "    900,\n",
    "    save_path,\n",
    "    True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    values = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                values.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    return values\n",
    "\n",
    "\n",
    "prefix = '/home/anton/h-elmo/expres/correlation/nocorrloss/sgd'\n",
    "path = 'tensors/valid/pickle_mean_tensors/correlation.pickle'\n",
    "dirs = [str(i) for i in range(3)]\n",
    "\n",
    "path_to_loss = os.path.join(prefix, '0/results/loss_valid.txt')\n",
    "\n",
    "def get_steps(fn):\n",
    "    steps = []\n",
    "    with open(fn) as f:\n",
    "        for line in f.readlines():\n",
    "            step = int(line.split()[0])\n",
    "            steps.append(step)\n",
    "    return steps\n",
    "\n",
    "steps = get_steps(path_to_loss)\n",
    "print(steps)\n",
    "        \n",
    "correlation_lines = OrderedDict(zip(['sgd'], [[]]))\n",
    "\n",
    "for d in dirs:\n",
    "    correlation_fn = os.path.join(prefix, d, path)\n",
    "    label = 'sgd'\n",
    "    y = load_pickle(correlation_fn)\n",
    "    y = [yy**0.5 for yy in y]\n",
    "    correlation_lines[label].append([steps, y])\n",
    "# correlation_lines['dropout 0.4'][0][1] = np.concatenate([a[:64], a[65:]])\n",
    "# m = float('inf')\n",
    "# for i in range(100):\n",
    "#     if abs(a[i] - a[i+1]) < m:\n",
    "#         m = abs(a[i] - a[i+1])\n",
    "#         j = i\n",
    "# print(j, abs(a[j] - a[j+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(prefix, 'plots/mean_sqr_correlation_bundles.png')\n",
    "\n",
    "plot_groups_of_lines(\n",
    "    correlation_lines,\n",
    "    {},\n",
    "    ['red', 'blue', 'black', 'brown'],\n",
    "    [],\n",
    "    0.4,\n",
    "    'step',\n",
    "    'mean square correlation',\n",
    "    'log',\n",
    "    'linear',\n",
    "    mpl.ticker.ScalarFormatter(),\n",
    "    0,\n",
    "    'best',\n",
    "    900,\n",
    "    save_path,\n",
    "    True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.int32(2*10**9)\n",
    "b = np.int32(10**9)\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  for layer in ['level0_0', 'level0_1']:\n",
    "    fn = tmpl.format(i, layer)\n",
    "    values = load_pickle(fn)\n",
    "    m = [np.min(v) for v in values]\n",
    "    for mm in m:\n",
    "      if mm < 0:\n",
    "        print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in {0..2}; do for l in \"${layers[@]}\"; do python3 $SCRIPTS/hist2entropy.py ${d}/${path}/hist_fixed^C{l}_hidden_state.pickle ${d}/${path}/entropy_${l}_hidden_state.pickle;     python3 $SCRIPTS/hist2mi.py ${d}/${path}/hist_fixed_${l}_hidden_state.pickle ${d}/${path}/cross_hist_fixed_${l}_hidden_state.pickle ${d}/${path}/mi_${l}_hidden_state.pickle; done; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.solvers import solve\n",
    "from sympy import *\n",
    "\n",
    "a, x = symbols('a x')\n",
    "sol = solve(0.5*x-0.25*sin(2*x)-a, x)\n",
    "\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from helmo.util.sampling import sample_hidden_sizes\n",
    "\n",
    "hidden_sizes = sample_hidden_sizes(321600, 0.9, 4, 100, 100)\n",
    "print(len(hidden_sizes))\n",
    "for hs in hidden_sizes:\n",
    "    print(sum(hs))\n",
    "print(hidden_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_lstm_param(input_size, hidden_size):\n",
    "    num_param = (input_size+hidden_size) * (4*hidden_size) + 4*hidden_size\n",
    "    return num_param\n",
    "\n",
    "\n",
    "def get_num_multi_lstm_param(input_size, hidden_sizes):\n",
    "    num_param = 0\n",
    "    for hs in hidden_sizes:\n",
    "        num_param += get_num_lstm_param(input_size, hs)\n",
    "        input_size = hs\n",
    "    return num_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hs in hidden_sizes:\n",
    "    print(hs, get_num_multi_lstm_param(100, hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "print(matplotlib.__version__)\n",
    "%matplotlib notebook\n",
    "\n",
    "points = []\n",
    "for _ in range(1000):\n",
    "    p = sample_point_from_sum_triangle(100, 4, 1)\n",
    "    if any([c < 0 for c in p]):\n",
    "        print('Error!', p)\n",
    "        break\n",
    "#     points.append(p)\n",
    "    \n",
    "# x, y, z = zip(*points)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(x, y, z)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "print(matplotlib.__version__)\n",
    "%matplotlib notebook\n",
    "\n",
    "from helmo.util.sampling import sample_point_from_sum_prism\n",
    "\n",
    "points = []\n",
    "for _ in range(1000):\n",
    "    p = sample_point_from_sum_prism(3, 1)\n",
    "#     print(p)\n",
    "    points.append(p)\n",
    "    \n",
    "x, y, z = zip(*points)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "points = sample_shifted_points_inside_sphere_with_constant_sum(\n",
    "    [100, 100, 100],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([9.346068550516723, -0.49577868439307227, 36.18069694773759, 54.96901318613876]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "psi_values = [sample_psi() for _ in range(100000)]\n",
    "plt.hist(psi_values, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "range_ = [-math.pi / 2, math.pi / 2]\n",
    "d = (range_[1] - range_[0]) / 400\n",
    "x = [range_[0] + d * i for i in range(400)]\n",
    "y = [0.5 * (xx + 0.5*math.sin(2*xx)) for xx in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "range_ = [-math.pi / 2, math.pi / 2]\n",
    "d = (range_[1] - range_[0]) / 400\n",
    "x = [range_[0] + d * i for i in range(400)]\n",
    "y = [-np.sin(xx) for xx in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "theta_values = [sample_theta() for _ in range(10000)]\n",
    "plt.hist(theta_values, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "range_ = [-math.pi / 2, math.pi / 2]\n",
    "d = (range_[1] - range_[0]) / 400\n",
    "x = [range_[0] + d * i for i in range(400)]\n",
    "y = [np.cos(xx) for xx in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "range_ = [-math.pi / 2, math.pi / 2]\n",
    "d = (range_[1] - range_[0]) / 400\n",
    "x = [range_[0] + d * i for i in range(400)]\n",
    "y = [np.cos(xx)**2 for xx in x]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import timeit\n",
    "\n",
    "t1 = timeit.timeit(\n",
    "    stmt=\"random.uniform(0,1)\",\n",
    "    number=1000,\n",
    "    globals=dict(random=random),\n",
    ")\n",
    "t2 = timeit.timeit(\n",
    "    stmt=\"random.uniform(0,100)\",\n",
    "    number=1000,\n",
    "    globals=dict(random=random),\n",
    ")\n",
    "print(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_net(start, end, factor):\n",
    "    net = []\n",
    "    while start < end:\n",
    "        net.append(start)\n",
    "        start *= factor\n",
    "        r_start = round(start)\n",
    "        if r_start <= start:\n",
    "            r_start = r_start+1\n",
    "        r_start = int(r_start)\n",
    "        start = r_start\n",
    "    net.append(end)\n",
    "    return net\n",
    "    \n",
    "    \n",
    "net = log_net(0, 80000, 1.0545)\n",
    "print(len(net))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from learning_to_learn.controller import Controller\n",
    "\n",
    "spec = {\n",
    "    \"type\": \"logarithmic_truth\",\n",
    "    \"start\": 0,\n",
    "    \"factor\": 1.02,\n",
    "    \"end\": 1000,\n",
    "}\n",
    "\n",
    "storage = {'step': 0}\n",
    "\n",
    "controller = Controller(storage, spec)\n",
    "\n",
    "count = 0\n",
    "for _ in range(1000):\n",
    "    if controller.get():\n",
    "        count += 1\n",
    "        # print(storage['step'])\n",
    "    storage['step'] += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def parse_pos_corr_table(table_file_name, layer_names):\n",
    "    with open(table_file_name) as f:\n",
    "        table_text = f.read()\n",
    "    text_by_tags = table_text.split('\\n')\n",
    "    text_by_tags = [line for line in text_by_tags if line]\n",
    "    data = OrderedDict(zip(layer_names, [[[], [], []] for _ in layer_names]))\n",
    "    for i, line in enumerate(text_by_tags):\n",
    "        values_and_errors = line.split(';')\n",
    "        for layer_name, vne in zip(layer_names, values_and_errors):\n",
    "            v, e = vne.split(' ± ')\n",
    "            v = float(v)\n",
    "            e = float(e)\n",
    "            data[layer_name][0].append(i)\n",
    "            data[layer_name][1].append(v)\n",
    "            data[layer_name][2].append(e)\n",
    "    return data\n",
    "\n",
    "\n",
    "data = parse_pos_corr_table(\n",
    "    'expres/resrnn/poscorr/4/tables/table.csv',\n",
    "    ['char encoder', 'char decoder', 'word encoder', 'word decoder'],\n",
    ")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)\n",
    "\n",
    "\n",
    "def mark_x_axis(tags):\n",
    "    n = len(tags)\n",
    "    plt.xticks(range(n))\n",
    "    set_size(12, 4)\n",
    "    plt.gca().set_xticklabels(tags)\n",
    "\n",
    "\n",
    "def read_tags(tag_file_name):\n",
    "    with open(tag_file_name) as f:\n",
    "        tags = f.readlines()\n",
    "    return [t.strip() for t in tags]\n",
    "\n",
    "\n",
    "def add_upper_x_axis(upper_tags):\n",
    "    xlim = plt.xlim()\n",
    "    secax = plt.twiny()\n",
    "    secax.set_xlim(*xlim)\n",
    "    secax.set_xticks(range(len(upper_tags)))\n",
    "    secax.set_xticklabels(upper_tags, rotation='vertical')\n",
    "\n",
    "\n",
    "def tag_plot(data, tags, colors, markers, xlabel, ylabel, upper_tags=None):\n",
    "    _, ax = plt.subplots()\n",
    "    for (label, dt), color, mk in zip(data.items(), colors, markers):\n",
    "        ax.errorbar(dt[0], dt[1], yerr=dt[2], color=color, marker=mk, label=label, linestyle=' ')\n",
    "    mark_x_axis(tags)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if upper_tags is not None:\n",
    "        add_upper_x_axis(upper_tags)\n",
    "    ax.grid()\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "\n",
    "tags = read_tags('dataset_tags/text8_first6.4e5_tags.txt')\n",
    "upper_tags = read_tags('dataset_tags/text8_first6.4e5_tag_description.txt')\n",
    "n = len(data['char encoder'][0])\n",
    "tag_plot(\n",
    "    data,\n",
    "    tags[:n],\n",
    "    ['red', 'green', 'blue', 'black'],\n",
    "    ['o', 'o', 'o', 'o'],\n",
    "    'part of speech',\n",
    "    'correlation',\n",
    "    upper_tags=upper_tags[:n],\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.6 -m pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(mpl.__version__)\n",
    "fig, ax = plt.subplots()\n",
    "print(type(ax))\n",
    "attributes = dir(ax)\n",
    "for m in attributes:\n",
    "    if 'secondary' in m:\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "r = tf.random.normal([3, 2])\n",
    "s = tf.summary.tensor_summary('r', r)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('testres/summary', filename_suffix='wow')\n",
    "    for i in range(1000):\n",
    "        writer.add_summary(sess.run(s), global_step=i)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "def get_event(dir_path):\n",
    "    return max(\n",
    "      glob.glob('{}/*'.format(dir_path)),\n",
    "                key=os.path.getctime)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.train import summary_iterator\n",
    "def get_lc(event_file):\n",
    "    lc = list(summary_iterator(event_file))\n",
    "    return(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests.integration.test_integration as mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/anton/test/test.txt', 'w') as f:\n",
    "    f.write(str('foo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod.get_number_from_file('/home/anton/test/test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import tests.utils_for_testing.tf_utils as tutil\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "func = tf.zeros\n",
    "args = [(3, 2)]\n",
    "q = mp.Queue()\n",
    "p = mp.Process(target=tutil.evaluate_tensor_in_sep_process, args=(q, func, args))\n",
    "p.start()\n",
    "print(q.get())\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/home/anton/h-elmo/expres/correlation/batch/nn_const/4/num_nodes_and_loss.csv',\n",
    ")\n",
    "fig = px.parallel_coordinates(df, color='loss', color_continuous_scale=px.colors.diverging.Tealrose,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/home/anton/h-elmo/expres/correlation/batch/nn_const/3/num_nodes_and_loss.csv',\n",
    ")\n",
    "fig = px.parallel_coordinates(df, color='loss', color_continuous_scale=px.colors.diverging.Tealrose,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/home/anton/h-elmo/expres/correlation/batch/nn_const/2/num_nodes_and_loss.csv',\n",
    ")\n",
    "fig = px.parallel_coordinates(df, color='loss', color_continuous_scale=px.colors.diverging.Tealrose,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plotly.io.orca.config.executable = '/home/anton/anaconda3/bin/orca'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/home/anton/h-elmo/expres/correlation/batch/nn_const/4/num_nodes_and_loss.csv',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=\n",
    "    go.Parcoords(\n",
    "        line = dict(color = df['loss'],\n",
    "                   colorscale = px.colors.sequential.Viridis,\n",
    "                   showscale = True),\n",
    "        dimensions = list([\n",
    "            dict(values=df['layer 1'], label='layer 1'),\n",
    "            dict(values=df['layer 2'], label='layer 2'),\n",
    "            dict(values=df['layer 3'], label='layer 3'),\n",
    "            dict(values=df['layer 4'], label='layer 4'),\n",
    "            dict(constraintrange = [1.22,1.225], values = df['loss'], label='loss'),\n",
    "        ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.write_image('/home/anton/h-elmo/expres/correlation/batch/nn_const/4/best.png')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plotly.io.orca.config.executable = '/home/anton/anaconda3/bin/orca'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/home/anton/h-elmo/expres/correlation/batch/nn_const/3/num_nodes_and_loss.csv',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=\n",
    "    go.Parcoords(\n",
    "        line = dict(color = df['loss'],\n",
    "                   colorscale = px.colors.sequential.Viridis,\n",
    "                   showscale = True),\n",
    "        dimensions = list([\n",
    "            dict(values=df['layer 1'], label='layer 1'),\n",
    "            dict(values=df['layer 2'], label='layer 2'),\n",
    "            dict(values=df['layer 3'], label='layer 3'),\n",
    "            dict(constraintrange = [1.255,1.27], values = df['loss'], label='loss'),\n",
    "        ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.write_image('/home/anton/h-elmo/expres/correlation/batch/nn_const/3/worst.png')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plotly.io.orca.config.executable = '/home/anton/anaconda3/bin/orca'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/home/anton/h-elmo/expres/correlation/batch/nn_const/2/num_nodes_and_loss.csv',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=\n",
    "    go.Parcoords(\n",
    "        line = dict(color = df['loss'],\n",
    "                   colorscale = px.colors.sequential.Viridis,\n",
    "                   showscale = True),\n",
    "        dimensions = list([\n",
    "            dict(values=df['layer 1'], label='layer 1'),\n",
    "            dict(values=df['layer 2'], label='layer 2'),\n",
    "            dict(constraintrange = [1.27,1.275], values = df['loss'], label='loss'),\n",
    "        ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.write_image('/home/anton/h-elmo/expres/correlation/batch/nn_const/2/best.png')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install psutil requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/bcdunbar/datasets/master/parcoords_data.csv\")\n",
    "print(df)\n",
    "\n",
    "fig = go.Figure(data=\n",
    "    go.Parcoords(\n",
    "        line = dict(color = df['colorVal'],\n",
    "                   colorscale = 'Electric',\n",
    "                   showscale = True,\n",
    "                   cmin = -1.22,\n",
    "                   cmax = -100),\n",
    "        dimensions = list([\n",
    "            dict(range = [32000,227900],\n",
    "                 constraintrange = [100000,150000],\n",
    "                 label = \"Block Height\", values = df['blockHeight']),\n",
    "            dict(range = [0,700000],\n",
    "                 label = 'Block Width', values = df['blockWidth']),\n",
    "            dict(tickvals = [0,0.5,1,2,3],\n",
    "                 ticktext = ['A','AB','B','Y','Z'],\n",
    "                 label = 'Cyclinder Material', values = df['cycMaterial']),\n",
    "            dict(range = [-1,4],\n",
    "                 tickvals = [0,1,2,3],\n",
    "                 label = 'Block Material', values = df['blockMaterial']),\n",
    "            dict(range = [134,3154],\n",
    "                 visible = True,\n",
    "                 label = 'Total Weight', values = df['totalWeight']),\n",
    "            dict(range = [9,19984],\n",
    "                 label = 'Assembly Penalty Wt', values = df['assemblyPW']),\n",
    "            dict(range = [49000,568000],\n",
    "                 label = 'Height st Width', values = df['HstW'])])\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "iris = px.data.iris()\n",
    "print(type(iris))\n",
    "fig = px.parallel_coordinates(iris, color=\"species_id\", labels={\"species_id\": \"Species\",\n",
    "                \"sepal_width\": \"Sepal Width\", \"sepal_length\": \"Sepal Length\",\n",
    "                \"petal_width\": \"Petal Width\", \"petal_length\": \"Petal Length\", },\n",
    "                             color_continuous_scale=px.colors.diverging.Tealrose,\n",
    "                             color_continuous_midpoint=2)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
